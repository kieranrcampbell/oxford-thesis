%********************************************************************
% Appendix
%*******************************************************
% If problems with the headers: get headings in appendix etc. right
%\markboth{\spacedlowsmallcaps{Appendix}}{\spacedlowsmallcaps{Appendix}}
\chapter{Model inference for switchde}

% Lorem ipsum at nusquam appellantur his, ut eos erant homero
% concludaturque. Albucius appellantur deterruisset id eam, vivendum
% partiendo dissentiet ei ius. Vis melius facilisis ea, sea id convenire
% referrentur, takimata adolescens ex duo. Ei harum argumentum per. Eam
% vidit exerci appetere ad, ut vel zzril intellegam interpretaris.
% \graffito{More dummy text.}

%Errem omnium ea per, pro congue populo ornatus cu, ex qui dicant
%nemore melius. No pri diam iriure euismod. Graecis eleifend
%appellantur quo id. Id corpora inimicus nam, facer nonummy ne pro,
%kasd repudiandae ei mei. Mea menandri mediocrem dissentiet cu, ex
%nominati imperdiet nec, sea odio duis vocent ei. Tempor everti
%appareat cu ius, ridens audiam an qui, aliquid admodum conceptam ne
%qui. Vis ea melius nostrum, mel alienum euripidis eu.


% Test: \autoref{tab:moreexample} (This reference should have a
% lowercase, small caps \spacedlowsmallcaps{A} if the option
% \texttt{floatperchapter} is activated, just as in the table itself
%  $\rightarrow$ however, this does not work at the moment.)

\section{Maximum likelihood model fitting} \label{app:switchdemle}

We begin with a $C \times G$ expression matrix $\bY$ for $G$ genes and $C$ cells with column vector $\by_g, g \in 1, \ldots, G$, that is non-negative and represents gene expression in a form comparable to $\log(\text{TPM} + 1)$. If the sigmoid function is defined as
\begin{equation}
	f(t_c; \muu, k_g, \tou) =
\frac{2 \muu}{1 + \exp\left( -k_g(t_c - \tou) \right)}
\end{equation}
then the likelihood of the data given the parameters is
\begin{equation}
	L(\by_g, \bm t; \muu, k_g, \tou) = \prod_{c = 1}^C \norm\left(y_{cg} | f(t_c; \muu, k_g, \tou), \sigma^2_g\right)
\end{equation}

We infer maximum likelihood estimates of the parameters using L-BFGS-B optimisation \cite{byrd1995limited} using the \texttt{R} function \texttt{optim}. This allows fast inference by passing analytical gradients as well as handling constraints on bounded variables. All parameters are defined on $\mathbb{R}$ except for $\mu_0$ and $\sigma^2$ which are optimised on $\mathbb{R}^+$. The parameters are initialised as follows: $\mu_0$ is set to $\frac{1}{C} \sum_c y_c$, $t_0 = \text{median}_c \;[ t_c]$, $\sigma^2 = \text{Var}_c [y_c]$. We initialise $k$ by using the gradient of the regression of $\by$ off $\bt$ to ensure the sign is correct.

We next need to compute the gradients for all parameters. If we consider the function (dropping $g$ subscripts)
\begin{equation}
	f(t_c; k, \mu_0, t_0) = \frac{2 \mu_0}{1 + \exp\left(-k(t_c - t^{(0)})\right)}
\end{equation}
(which we may write succinctly as $f(t_c; \Theta)$ where $\Theta = \{ \mu_0, k, t^{(0)} \}$) then the partial derivatives are given by

\begin{equation} \label{eq:derivatives}
\begin{aligned}
\frac{\partial f}{\partial \mu_0} & = \frac{2}{1 + \exp\left(-k(t_c - t^{(0)})\right)} \\
\frac{\partial f}{\partial k} & = \frac{ f(t_c; \Theta) (t_c - t^{(0)})}{1 + e^{k(t_c - t^{(0)})}} \\
\frac{\partial f}{\partial t^{(0)}} & = \frac{-k f(t_c; \Theta)}{1 + e^{k(t_c - t^{(0)})}}
\end{aligned}
\end{equation}

To find the maximum likelihood estimate of $\Theta \equiv (\mu_0, k, t^{(0)})$ we wish to minimise the negative log-likelihood, given by

\begin{equation}
\begin{aligned}
\mathcal{L}  = - \log L(\by, \bt; \Theta) & = -\sum_{c=1}^C \log \mathcal{N}\left(y_c | f(t_c; \Theta), \sigma^2 \right) \\
& = -\sum_c \left[ \log \frac{1}{\sqrt{2 \pi \sigma^2}} - \frac{1}{2 \sigma^2} \left( y_c - f(t_c; \Theta) \right)^2\right] \\
& = C \log \sqrt{2 \pi \sigma^2} + \frac{1}{2 \sigma^2} \sum_c \left( y_c - f(t_c; \Theta) \right)^2
\end{aligned}
\end{equation}

Then to compute the gradient of $\mathcal{L}$ with respect to a given parameter $\theta \in \Theta$ it follows that


\begin{equation}
\frac{d\mathcal{L}}{d \theta} = \frac{1}{2 \sigma^2} \sum_c
\left[ -2 \left( y_c - f(t_c; \Theta) \right) \frac{\partial f(t_c; \Theta)}{\partial \theta} \right]
\end{equation}


Note that this is general to any iid Gaussian measurements with a parametric mean function.

Further, since we have
\begin{equation}
\mathcal{L} \propto \frac{C}{2} \log \sigma^2 + \frac{1}{2 \sigma^2}
\sum_c \left( y_c - f(t_c; \Theta) \right)^2
\end{equation}

it follows that
\begin{equation}
\frac{d\mathcal{L}}{d \sigma^2} = \frac{C}{2 \sigma^2} - \frac{1}{2 \sigma^4} \sum_c \left( y_c - f(t_c; \Theta) \right)^2.
\end{equation}

\section{Expectation-Maximisation for zero inflation} \label{app:switchdeem}

Single-cell RNA-seq data is known to exhibit an over-representation of zeros known as ``dropouts''. To account for this we propose a model that incorporates dropouts in a similar style to \cite{pierson2015zifa}. Our model becomes

\begin{equation}
\begin{aligned}
\mu(t_c, \theta) & = \frac{2 \mu_0}{1 + \exp\left(-k(t_c - t^{(0)})\right)} \\
x_{c} & \sim \mathcal{N}(\mu(t_c, \theta), \sigma^2) \\
h_{c} | x_{c} & \sim \mathrm{Bernoulli}(\exp(-\lambda x_{c}^2)) \\
    y_{c} &=
\begin{cases}
    x_{c} ,& \text{if } h_{c} = 0\\
    0,  & \text{if } h_{c} = 1
\end{cases}
\end{aligned}
\end{equation}

Where we have replaced $f \rightarrow \mu$ to avoid cluttering notation later. We essentially introduce a latent variable $x_c$ for each gene expression measurement but must now perform inference using the Expectation-Maximisation (EM) algorithm due to the intractability of directly maximising the log-likelihood. The secondary latent variable $h_c$ is a binary indicator for whether the expression measurement in cell $c$ is dropout or not. In the following we derive the EM algorithm which follows a similar derivation to \cite{pierson2015zifa} but with some differences, so it is provided in full below.

In the following let $\Theta = \{\mu_0, k, t^{(0)}, \sigma^2\}$ and consider the complete-data likelihood:

\begin{equation}
	\begin{aligned}
p(\by, \bx, \bh, \Theta) & = \prod_c p(y_c, x_c, h_c, \Theta), \\
& = \prod_c p(y_c | x_c, h_c) p(h_c | x_c, \Theta) p(x_c | \Theta), \\
	p(y_c, x_c, h_c, \Theta) & =
	\begin{cases}
		 (1 - e^{-\lambda x_c^2}) p(y_c | x_c, h_c = 0)  p(x_c | \Theta), & h = 0, \\
		 e^{-\lambda x_c^2} p(y_c | x_c, h_c = 1)  p(x_c | \Theta) , & h = 1. \\
	 \end{cases}
\end{aligned}
\end{equation}


We then use the same trick as \cite{pierson2015zifa}: if $y_c = 0$ then we know necessarily that $h_c = 1$ as $h_c = 0$ with zero probability. Similarly, if $y_c > 0$ then we observe $x_c = y_c$ and know that $h_c = 0$. We therefore split the product up into terms involving $y_c = 0$ and those involving $y_c > 0$, and now consider the log of the complete data likelihood with the shorthand notation $\mu_c \equiv \mu(t_c, \theta)$:

\begin{equation} \label{eq:lcl}
\begin{aligned}
\mathcal{L}(\by, \bx, \bh, \Theta) & = \sum_\cz \left[ \log \norm(x_c | \mu_c, \sigma^2) - \lambda x_c^2 \right] + \sum_\cnz \left[ \log \norm(y_c | \mu_c, \sigma^2) + \log(1 - e^{-\lambda y_c^2}) \right] \\
& = \frac{-C}{2} \log(2 \pi \sigma^2) +
\sum_\cz \left[ \frac{(x_c - \mu_c)^2}{2 \sigma^2} - \lambda x_c^2 \right] +
\sum_\cnz \left[ \frac{(y_c - \mu_c)^2}{2 \sigma^2} + \log(1 - e^{-\lambda y_c^2}) \right] \\
& = \frac{-C}{2} \log(2 \pi \sigma^2) +
\sum_\cz \left[ -(\frac{1}{2\sigma^2} + \lambda) x_c^2 + \frac{\mu_c}{\sigma^2} x_c - \frac{\mu_c^2}{2\sigma^2}\right] +
\sum_\cnz \left[ \frac{(y_c - \mu_c)^2}{2 \sigma^2} + \log(1 - e^{-\lambda y_c^2}) \right]
\end{aligned}
\end{equation}

In order to perform EM we need to calculate the expected value of this log likelihood, conditional on the data $\by$ and a previous estimate $\Thetat$:

\begin{equation}
Q(\Theta|\Thetat) = \Ex_{\bx | \by, \Thetat} [\mathcal{L}(\by, \bx, \bh, \Theta)]
\end{equation}

In order to calculate this it is obvious from equation \ref{eq:lcl} we must calculate $\Ex_{\bx | \by, \Thetat} [ x_c ]$ and $\Ex_{\bx | \by, \Thetat} [ x_c^2 ]$. Notice we only care about $\cz$ since for $\cnz$ we know $x_c$ exactly. Note that in all the following all the parameters are assumed fixed at the previous iteration, e.g. $\mu_c \equiv \mu_c^{(t)}$. If we consider a conditional density of the form

\begin{equation}
f(\bx | \by, \Thetat) = \prod_c f(x_c | y_c, \Thetat)
\end{equation}

then

\begin{equation}
\begin{aligned}
f(x_c | y_c, \Thetat) & = \frac{f(y_c | x_c, \Thetat)f(x_c | \Thetat)}{\int dx_c f(y_c | x_c, \Thetat)f(x_c | \Thetat)} \\
& = \frac{e^{-\lambda x_c^2} \norm(x_c | \mu_c, \sigma^2)}{\int dx_c e^{-\lambda x_c^2} \norm(x_c | \mu_c, \sigma^2)}
\end{aligned}
\end{equation}


Some algebra later and we arrive at

\begin{equation}
f(x_c | y_c, \Thetat) = \norm\left(x_c | \alpha(t_c, \Thetat), \beta(\Thetat) \right)
\end{equation}

where

\begin{equation}
\begin{aligned}
\alpha(t_c, \Thetat) & = \frac{\mu_c}{2 \sigma^2 \lambda + 1} \\
\beta(t_c) & = \frac{\sigma^2}{2 \sigma^2 \lambda + 1}
\end{aligned}
\end{equation}

and so

\begin{equation}
\begin{aligned}
\Ex_{\bx | \by, \Thetat} [ x_c ] &= \alpha(t_c, \Thetat)  \\
\Ex_{\bx | \by, \Thetat} [ x_c^2  ] &= \alpha(t_c, \Thetat)^2 + \beta(\Thetat).
\end{aligned}
\end{equation}

We need to maximise

\begin{equation}
\begin{aligned}
Q(\Theta | \Thetat) &= \frac{-C}{2} \log(2 \pi \sigma^2) \\
& +
\sum_\cz \left[ -(\frac{1}{2\sigma^2} + \lambda) \Ex_{\bx | \by, \Thetat} [ x_c^2 ] + \frac{\mu_c}{\sigma^2} \Ex_{\bx | \by, \Thetat} [ x_c ] - \frac{\mu_c^2}{2\sigma^2}\right] \\
& +
\sum_\cnz \left[ \frac{(y_c - \mu_c)^2}{2 \sigma^2} + \log(1 - e^{-\lambda y_c^2}) \right]
\end{aligned}
\end{equation}

with respect to $\theta = \{\mu_0, k, t_0\}$, $\sigma^2$ and $\lambda$, recalling $\mu_c \equiv \mu_c(\theta, t_c)$. We wish to use gradient-based optimisation and so require the gradients. Note that

\begin{equation}
\begin{aligned}
\frac{dQ}{d\theta} &= \sum_c \frac{\partial Q}{\partial \mu_c} \frac{d \mu_c}{d \theta} \\
& = \frac{1}{\sigma^2} \left[ \sum_\cz \left(  \Ex_{\bx | \by, \Thetat} [ x_c ] - \mu_c \right) \frac{d \mu_c}{d \theta} +
\sum_\cnz \left( y_c - \mu_c \right) \frac{d \mu_c}{d \theta}\right]
\end{aligned}
\end{equation}

where the derivatives $\frac{d \mu_c}{d \theta}$ are the same as those given in equation \ref{eq:derivatives}. Finally we require the partial derivatives with respect to $\lambda$ and $\sigma^2$ which are given by

\begin{equation}
\frac{dQ}{d \lambda} = -\sum_\cz  \Ex_{\bx | \by, \Thetat} [ x_c ] +
\sum_\cnz \frac{y_c^2 e^{-\lambda y_c^2}}{1 - e^{-\lambda y_c^2}}
\end{equation}

and

\begin{equation}
\frac{dQ}{d\sigma^2} = \frac{-C}{2\sigma^2} +
\frac{1}{2 \sigma^4} \left[ \sum_\cz \left( \Ex_{\bx | \by, \Thetat} [ x_c^2 ] - 2 \mu_c \Ex_{\bx | \by, \Thetat} [ x_c ] + \mu_c^2 \right) +
\sum_\cnz (y_c - \mu_c)^2 \right]
\end{equation}

Note that $\sigma^2$ has an analytical maximum by setting $\frac{dQ}{d\sigma^2} = 0$, but since this depends on $\theta$ and vice versa we instead numerically optimise all simultaneously.
