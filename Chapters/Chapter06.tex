%************************************************
\chapter{Conclusion}\label{ch:conclusionchap} % $\mathbb{ZNR}$
%************************************************

In this thesis we have considered inferring ``trajectories'' through gene expression space as a statistical latent variable problem. Such approaches are feasible when the data is gene expression from single-cells that undergo a physical time process where it is difficult to truly measure time and population-level cancer studies where such trajectories correspond to the activation of biological pathways. 

As such quantities are estimated from noisy biological data it is important that a full probabilistic treatment taking uncertainty into account is considered as this has a large impact on downstream differential expression testing. We further demonstrated that contrary to existing approaches, which typically employ dimensionality reduction and cell ordering steps, it is possible to learn pseudotimes from a small set of marker genes using a simple parametric model. In such settings if the genes exhibit ``switch-like'' changes in expression over pseudotime then encoding this information as informative Bayesian priors typically improves inference.

We further considered the case of bifurcations in single-cell data, where cells proceed along some developmental trajectory before undergoing a fate decision that leads to two or more possible outcomes. All existing approaches to date treat this as a two step procedure that first learn a pseudotime for each cell then identify branching post-hoc assuming the pseudotimes are fixed. This has obvious issues in terms of treating the pseudotimes as fixed quantities and assumes the major source of variation comes from the pseudotemporal progress rather than the branch structure. We proposed that a Bayesian mixture of factor analysers is a solution to jointly generatively model both the trajectory and bifurcation event. By imposing a unique hierachical structure between the loading matrices of each mixture we were able to automatically identify genes that bifurcated, though this was limited to an extent by the linear assumptions of the model.

Finally we considered the case where such trajectories exist against a heterogeneous genetic or environmental background. We proposed a novel type of latent variable model that allows the behaviour of features to vary along the trajectory differently depending on some externally measured quantity, such as mutational burden or stimulant exposure. We further applied this to population wide cancer gene expression data, demonstrating that such ``trajectories'' correspond to biological pathway activation and identified interactions between such pathways and externally measured covariates. A fast variational inference algorithm was derived in order to infer such trajectories and interactions for thousands of genes and samples relatively quickly. We further proposed an extension for the case that the externally measured covariate is a possibly-censored survival time. Finally, we derived a non-parametric extension termed a covariate-adjusted Gaussian process latent variable model and demonstrated its utility on some synthetic data.

There are several ideas to be considered that form extensions to this work and the basis for new research. A major theme in our discussions so far and indeed among the current single-cell community is scaling up inference. This is particularly pertinent as new technologies such as Drop-seq and 10x genomics scale up sequencing to hundreds of thousands and millions of cells\footnote{
   Gone are the early-PhD days of emailing yourself a single-cell dataset and pretending to everyone else that you work on ``Big Data''.
}. 

A major advantage of phrasing the pseudotime estimation problem as a (Bayesian) statistical latent variable one is that once we have written down a model we can ``borrow'' scalable inference procedures from elsewhere in the field. This is in contrast to what you could term the more algorithmic approach to pseudotime inference - that often relies on some tree or graph-building procedure - where a hand-crafted computational considerations are required for each algorithm and modification.

An obvious future direction for such research is to construct Bayesian models that use (black-box) variational inference\footnote{
    To quote David Blei, ``Variational inference is that thing you implement while waiting for your Gibbs sampler to converge.''
} and stochastic variational inference (SVI). The major advantage behind SVI is that it can subsample observations typically in batches in order to climb noisy estimates of the ELBO, meaning it scales well as we vastly increase the number of cells in our datasets. An interesting observation is that statistical modelling of gene expression has typically been a $P \gg N$ problem, as we have $P \approx 20,000$ genes but $N \sim \mathcal{O}(10)$ samples. However, as the single-cell library preparation technology improved, we now face the opposite situation: $P$ will always be fixed\footnote{
Alternatively if you look at expression of individual transcripts rather than genes then $P \approx 170,000$ but as before this is fundamentally fixed.
} at $\approx 20,000$ (barring some miracle of nature), while $N$ is  unbounded in practice.

The scalability issue becomes worse if we work with Gaussian processes, where just evaluating the likelihood requires an expensive $\mathcal{O}(N^3)$ matrix inversion. A promising future direction in the field would then look at scalable inference for GPs in single-cell transcriptomics using techniques such as inducing points.

- Smoothing in latent variable models
- Sensitivity of MSTs and relations to current approach
- Multiview learning
- Experimental validation
- Calibrating latent variables in population level datasets


%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
