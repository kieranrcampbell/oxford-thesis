%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

\section{Single-cell RNA-sequencing}

\subsection{Why quantify gene expression in single-cells?}

\subsubsection{Central dogma}

\subsubsection{Heterogeneity at the single-cell level}


\subsection{Bulk expression quantification}

\subsubsection{rt-qpcr}


\subsubsection{Microarrays}


Microarrays preceded RNA sequencing as the transcriptome-wide quantification method of choice. To infer nucleic acid abundance, DNA fragments (after reverse transcription from mRNA transcripts for gene expression quantification) bind to ``probes'' of known nucleic acid sequence and fluoresce strongly if a large quantity of nucleic acid with the correct complementary sequence is present.

\subsubsection{RNA sequencing}


\subsection{Single-cell expression quantification methods}

\subsection{Features of scRNA-seq data}

\subsubsection{ Mean-variance relationship}

\subsubsection{Dropout}

\section{Pseudotime \& trajectories}

\subsection{The pseudotime estimation problem}

In many single-cell assays cells undergo some transformation over time. Examples include the differentiation of stem cells into neurons or skin cells, cells progressing through the cell cycle, or cells undergoing apoptosis (cell death). Ideally we would like to track expression changes over time, revealing key gene expression changes that correlate with the biological progression of interest.

However, most gene expression quantification methods to date - particularly transcriptome-wide ones such as single-cell RNA sequencing - destroy the cell during the measurement process, thus prohibiting repeated time-series measurement on the same cell. A first solution would be to repeatedly measure sets of cells at different time points, analogous to bulk RNA sequencing. However, the transcriptional heterogeneity at the single-cell level leads to an asynchronicity of expression \cite{Trapnell2014-xi}, meaning some cells at a given time point will be transcriptionally more similar to those at the next and some transcriptionally more similar to those at the previous.

\begin{figure}
\centering
  \includegraphics[width=0.98\textwidth]{gfx/ch1/figure1.png}
  \caption{The pseudotime estimation problem.
\textbf{A} Cells undergo some physical time process such as differentiation, cell-cycle, or apoptosis.
\textbf{B} The cell capture and expression quantification method - such as single-cell RNA-seq or single-cell mass-cytometry - leads to a loss of temporal information (if it were possible to measure in the first place).
\textbf{C} Pseudotime algorithms attempt to reconstruct the original time series using gene expression data alone.
\textbf{D} Downstream analysis such as differential expression proceeds with the pseudotimes in place of the physical times.
  } \label{fig:pseudotime}
\end{figure}

As a solution to these issues the idea of assigning cells a \emph{pseudotime} was proposed. In such a setting an algorithm takes the transcriptome-wide gene expression measurements for all cells and maps each on to a one-dimensional pseudotime\footnote{Mathematically, a pseudotime algorithm is just a function $f: \mathbb{R}^{N\times G} \rightarrow \mathbb{R}^N$ for $N$ cells and $G$ genes, mapping the gene expression matrix to a vector of pseudotimes (one for each cell). Strictly it is not a function $f: \mathbb{R}^G \rightarrow \mathbb{R}$ since the pseudotime for a given cell is (typically) dependent on all other cells. Such an explicit functional form reveals that pseudotime algorithms are in fact just a form of dimensionality reduction.}.
The pseudotime of a particular cell quantitatively represents that cell's progression through the biological process of interest and is not necessarily supposed to represent the physical (capture) time due to the transcriptional asynchronicity mentioned above. Consequently, each cell acts as a surrogate time point, the transcriptome of a given cell representing the supposed transcriptional signature of the biological process at that cell's assigned (pseudo-)time point. The set of cells ordered by pseudotime is often described as a \emph{trajectory} representing the continuous progression of transcription along the process of interest (figure \ref{fig:pseudotime}).

Several downstream analysis typically follow the assignment of pseudotimes to cells. Examples include identifying genes differentially expressed along the trajectory \cite{campbell2016switchde} or gene clusters differentially regulated \cite{Trapnell2014-xi}. However, care must be taken to realise that the pseudotimes are an uncertain estimate derived from exceptionally noisy input data and not a ``perfectly'' measured quantity such as physical capture time; this is the subject of chapter 2.

\subsection{Early applications to bulk expression data} \label{sec:int:early}

The ideas behind pseudotime orderings were first introduced in the context of ordering bulk microarray samples by Magwene et al. in 2002 \cite{Magwene2003-bm}. They examine the case of tracking gene expression from tumour samples in mouse models from disease inception and argue that ``the samples obtained from a mouse model of cancer do not represent a time series with a well-defined developmental order''
because early in cancer multiple tumours within the same tissue may progress asychronously. Magwene et al. therefore suggest that obtaining an ordering of samples based on the gene expression measurement alone may more accurately depict cancer development.

Their algorithm is motivated by the fact that the noise-free progression of microarray samples would trace out a smooth one-dimensional curve embedded in high dimensional expression space. They begin by fitting a minimum spanning tree\footnote{
A minimum spanning tree is a graph that connects all vertices together without any cycles using the minimum overall edge weight, a little like joining all the dots using the least ink possible.
} (MST) to the data using a modified distance function that is the ``standard pairwise dissimilarity'' if two points are ``relatively similar'' and the sum of the pairwise dissimilarities between two samples if ``relatively dissimilar''.

If the MST represents a path (i.e. it has no branches) then this is taken to be the ordering of cells. However, if the MST does have branches then the ``diameter path'' (the longest path through the MST) is computed and various heuristics are run to assess whether the diameter path essentially represents the dominant mode of variation through the data. If so then the diamater path is taken to be the ordering and if not a set of orderings representing the uncertainty in path variations is returned. Magwene et al. applied this algorithm to time series data of bacterial gene expression and found that it reconstructed the true time ordering of the samples accurately.

Gupta and Bar-Joseph (2008, \cite{Gupta2008-fd}) expanded on the ideas of Magwene et al. in three important ways. Firstly, they formally proved that a method could reconstruct the temporal order of expression profiling measurements. Interestingly, this proof shows that expression profiles of samples near each other in time will be more similar than those further apart in time, provided the change in expression is correlated over time\footnote{i.e. if a gene is upregulated at a given time point, it is more likely to be upregulated at the next and vice versa.} with high probability.
Secondly, they applied their method to static cancer data rather than simply recovering the capture time from time-series microarrays, though in practice the algorithm of Gupta et al. could be applied here also. Finally, their method recovered expression profiles for individual genes using spline fits. Interestingly, their model is close to a nonlinear factor analysis model like that considered in chapter 3, but performs a line search at each iteration of an expectation-maximisation (EM) algorithm to find the pseudotimes of each sample that minimises the mean squared error, rather than maximum likelihood or Bayesian inference of the pseudotimes.

Following this was the introduction of \emph{Sample Progression Discovery} (SPD) by Qui et al. in 2011 \cite{Qiu2011-ol}. This built on similar ideas of using MSTs to connect together microarray samples in a time ordering, with several important differences. SPD performs consensus clustering on the expression matrix to identify correlated gene expression modules and calculates a MST for each of them. It then chooses a subset of gene modules that have concordant MSTs and calculates an overall progression MST using only these. The authors emphasise that such an approach provides a feature selection ability, highlighting modules of genes associated with changes in the MST.


% \subsection{Single-cell trajectories}

\subsection{Single-cell pseudotime inference algorithms}

\subsubsection{Overview}

Ordering bulk microarray samples was arguably a niche area, with the three studies discussed above constituting the main work in the field. However, the advent of single-cell sequencing lead to an explosion of interest in the field, mostly due to the higher inter-sample heterogeneity present in single-cell data. The first single-cell pseudotime algorithm was published in March 2014 \cite{trapnell2014dynamics}; at time of writing (May 2017) there are now approximately 33 single-cell pseudotime inference algorithms that have at least been preprinted\footnote{
A comprehensive spreadsheet of single-cell software can be found at \url{https://tinyurl.com/mqabyur}.
}, a rate of roughly one per month.

\begin{sidewaystable}
  \centering
\begin{tabular}{|lccccc|}
\hline
Algorithm & Reference & Dimensionality reduction & Cell ordering & Probabilistic & Branching \\
\hline
Monocle & \cite{Trapnell2014-xi} & ICA\footnote{Independent component analysis} &
MST\footnote{Minimum spanning tree (cells are projected onto the longest path through the MST).} & No & Yes \\
Wanderlust & \cite{Bendall2014-rc} & N/A & KNN-graph\footnote{$k$-nearest neighbour graph. Trajectory inferred by an ensembl of random walks.} & No & No \\
Monocle 2 & \cite{Qiu2017-eu} & DDR-tree & Distance from root cell & No & Yes \\
Scuba & \cite{Marco2014-ug} & t-SNE\footnote{See section \ref{sec:tsne}} & Principal curves & No & Yes \\
Diffusion pseudotime (DPT) & \cite{Haghverdi2016-eg} & N/A & Diffusion distance from root cell &
Interpretation\footnote{DPT has an interpretation in terms of the probability of one cell state transitioning into the other. However, it does not define a generative probabilistic model.} & Yes \\
SLICER & \cite{welch2016slicer} & Locally linear embedding\footnote{See section \ref{sec:lle}} & Principal curves & No & Yes \\
DeLorean & \cite{reid2016pseudotime} & N/A & GPLVM\footnote{See section \ref{sec:gplvm}} & Yes & No \\
TSCAN & \cite{Ji2016-gx} & PCA & Cluster-based MST & No & Yes \\
Waterfall & \cite{shin2015single} & PCA & Cluster-based distances & No & No \\
\rowcolor{Gray}
Ouija & \cite{Campbell2016-ys} & N/A & Nonlinear factor analysis & Yes & No \\
\rowcolor{Gray}
MFA & \cite{campbell2017probabilistic} & N/A & Mixture of factor analysers & Yes & Yes \\
\hline
\end{tabular}
\caption{An overview of some pseudotime algorithms. Most involve a dimensionality reduction step followed by pseudotime assignment (``cell ordering'') in the reduced space, though arguably this constitutes a single dimensionality reduction step. Methods shaded in grey are introduced in this thesis.} \label{tbl:pseudotimecomparison}
\end{sidewaystable}

This renaissance was largely spurred by the development of \texttt{Monocle} \cite{trapnell2014dynamics}, discussed in detail in section \ref{tbl:monocle}. Monocle's two step procedure - an initial dimensionality reduction step using independent component analysis followed by a ``cell ordering'' step to order the cells using MSTs - greatly influenced the algorithms that followed. Examples include \texttt{TSCAN} that uses PCA for dimensionality reduction followed by clustering and MSTs for cell ordering; \texttt{embeddr} that uses laplacian eigenmaps for dimensionality reduction and principal curves for cell ordering; and \texttt{Waterfall} that uses PCA for dimensionality reduction followed by connecting clusters in the reduced space for cell ordering. An overview of some pseudotime algorithms is given in table \ref{tbl:pseudotimecomparison} with several important contributions discussed in detail in the following sections.

\subsubsection{Monocle} \label{tbl:monocle}

Monocle was designed to understand the gene expression dynamics that accompany the differentiation of stem cells into human skeletal muscle myoblasts (HSMM). There experimental design was to take stem cells and engineer a serum switch to induce differentiation, then perform single-cell RNA-sequencing at 0h, 24h, 48h and 72h afterwards. The authors noted that the expression patterns of key marker genes over time (such as switch-like inactivation of \emph{ID1}) was absent, and suggested asynchronicity of cellular development could be the underlying cause and thus a pseudotemporal ordering of cells was required.

Monocle begins with a gene selection step to identify which should be retained for ordering the cells. In the original publication the authors used those genes differentially expressed\footnote{
Using a Tobit likeihood model; see chapter 2 for further discussions on differential expression over pseudotime.
} between the time points, though emphasise that other approaches (such as selecting highly variable genes) would work also.

\begin{figure}
\centering
  \includegraphics[width=0.98\textwidth]{gfx/ch1/monocle.png}
  \caption{The Monocle pseudotime algorithm (reused with permission).
\textbf{a} Dimensionality reduction is performed on the expression matrix using ICA. A MST is then fitted in the reduced space and cells are ordered along the diameter path, with branching also detected. Downstream analysis then includes differential expression and gene clustering.
\textbf{b} Monocle applied to human skeletal muscle myoblasts. The authors identified three cell types (proliferating cells, differentiating myoblasts, and interstitial mesenchymal cells). The main pseudotime trajectory progresses from the proliferating cells to the differentiating myoblasts, with the contaminating mesenchymal cells appearing on a separate branch.
  } \label{fig:monocle}
\end{figure}

The algorithm proceeds by using independent component analysis (ICA) to reduce the dimensionality of the dataset down to two dimensions. ICA attempts to find latent components in the data that are statistically independent and assumes that each component is non-Gaussian\footnote{
ICA finds a projection that maximises the kurtosis in the latent space, compared to PCA that maximises the variance (see section \ref{sec:intr:pca}).
}. The original paper provides no justification for using ICA over any other dimensionality reduction algorithm, nor the choice of two latent dimensions though they do note that this makes it easier to visualise and interpret. Interestingly, the authors note

\begin{displayquote}
\emph{Reducing dimensionality of single-cell expression data amounts to describing
each cell in terms of abstract sources, which are hidden variables that
describe a cell’s state but which are reflected in observed gene expression
values.}
\end{displayquote}

which would seemingly justify a single dimensionality reduction step down to a one-dimensional latent component as a valid pseudotime algorithm.

However, the Monocle algorithm proceeds with a secondary cell ordering step in the reduced space. It largely follows the procedure of Magwene et al. \cite{Magwene2003-bm} (see section \ref{sec:int:early}) in constructing a MST in the reduced space and finding the ``diameter path'' (i.e. the longest path through the MST) as the trajectory. In order to find biological branches (rather than technical ones), the algorithm finds branches from ``indecisive'' vertices (those with degree greater than two) and returns the $k$ longest, where $k$ is an integer set by the user in line with prior knowledge of the expected number of terminally differentiated cells in the sample.

\subsubsection{Wanderlust}

Published just a month after Monocle was Wanderlust \cite{Bendall2014}, an algorithm developed to uncover human B cell lymphopoiesis using single-cell mass cytometry data. Compared to transcriptome wide RNA sequencing, mass cytometry measures a smaller number of markers. In this study, a custom panel of 44 makers designed to characterise B-cells were measured, including ``phenotypic proteins, transcription factors, regulatory enzymes, cell-state indicators, and activation of regulatory signaling molecules''.

Wanderlust begins by constructing a $k$-nearest neighbour graph between all cells using euclidean distance and selects a small random subset of cells as ``waypoints''. It then creates a random ensembl of graphs by subsampling cells to mitigate the impact of ``short circuits'' on the trajectory construction\footnote{
Short circuits are defined as ``spurious edges between distant cells'' - two cells distant in pseudotime are connected due to random fluctuations in their gene expression. In theory such a problem will be worse in the ~40 dimensional mass cytometry data than 1000+ dimensional single-cell RNA-seq.
}. Wanderlust then assigns each cell an initial time as the shortest path through the graph from a manually chosen ``early'' cell, and performs an iterative procedure where each cells pseudotime is a weighted average of the distances to the waypoint cells, which is repeated until convergence. The overall pseudotime for each celll is taken as the average over   the ensembl of graphs.

\subsubsection{DeLorean}

DeLorean \cite{reid2016pseudotime} was published approximately two years after Monocle and Wanderlust and is mentioned here as an example of an entirely different method. DeLorean departs from previous approaches in using a probabilistic model called a Gaussian Process Latent Variable Model (GPLVM, discussed in section \ref{sec:gplvm} and the subject of most of chapter 2).

The GPLVM learns a probabilistic mapping from the one dimensional latent pseudotimes to the observed gene expression profiles. The basic idea is that if two cells have similar pseudotimes then their transcriptomes will be highly correlated and thus will have similar measured expression profiles\footnote{
This is essentially a continuity assumption - that cells close in pseudotime will have expression profiles ``close'' in expression space. This assumption was discussed in \cite{Gupta2008-fd} (see section \ref{sec:int:early}) who showed the change in expression over (pseudo-)time must be correlated for it to be true. This logic underlies many if not all pseudotime inference methods, such as DPT \cite{Haghverdi2016-eg}.
}. Inference in the DeLorean model proceeds using the Stan probabilistic programming language, which performs automatic Bayesian inference using either Hamiltonian Monte Carlo (HMC) or Automatic-Differentiation Variational Inference (ADVI).

DeLorean notes several drawbacks, some of which  we also grapple with in chapter 2. It may only be used on time-series single-cell expression profiles, and thus in a sense is used to refine time series rather than a strict pseudotime algorithm. If $k_n$ is the capture time of cell $n$, then the prior on $n$'s pseudotime $t_n$ takes the form $t_n \sim \Norm(k_n, \sigma_n^2)$, so the confidence in the capture time $\sigma_n$ must also be set by the user. Furthermore, the characteristic length scale of the GPLVM kernel (see section \ref{sec:gplvm}) must be manually set by the user, and even so the posterior distribution of pseudotimes is multi-modal. Finally, GPLVM requires parameters for each output (gene) which can scale poorly, so in practice pseudotime fitting is limited to a small number of genes chosen \emph{a priori}.

However, a strength of DeLorean is the ability to discuss these drawbacks because of its formulation as a probabilistic model. Given the underlying continuity assumption common to all pseudotime algorithms we would expect they are all multimodal to some extent, or at least have multiple solutions that are dependent on algorithm parameters or initial values. However, their ad-hoc algorithmic nature precludes any assessment of this, while in the DeLorean model these can be recognised as a multimodal posterior distribution and steps tacken to mitigate this such as the systematically incorporating prior information in the form of capture times to constrain the latent space.




\section{Statistical latent variable models}

\subsection{Principal component analysis} \label{sec:intr:pca}

Principal component analysis (PCA, \cite{jolliffe2002principal}) is a ubiquitous linear dimensionality reduction technique. PCA has several interpretations\footnote{Lior Pachter has an excellent blog post on this at \url{https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/}}, but the most common is that of a linear projection to a low-dimensional space such that the variance of the data points in the projected space is maximised. Intuitively, this can be thought of finding a linear subspace of the high-dimensional data space that best ``explains''
the data.

Mathematically, we start with $G$-dimensional data points $\{\mby_n\}, \; n = 1, \ldots, N$ and wish to find the $Q$ principal axes $\mblambda_q, \; q = 1, \ldots, Q$ that act as a mapping from the observed to latent space such that the variance of the input points projected to the latent space is maximal. To find $\mblambda_q$ the eigenvectors of the sample covariance matrix

\begin{equation}
  \mbS = \frac{1}{N}\sum_{n = 1}^N (\mby_n - \bar{\mby}) (\mby_n - \bar{\mby})^T
\end{equation}

are calculated where $\bar{\mby}$ is the sample mean of $\mby_n$. The $Q$ principal axes are then the $Q$ eigenvectors of $\mbS$ with the largest eigenvalues. The latent space projections $\mbz_n$ of each data point may then be obtained via $\mbz_n = \mbLambda (\mby_n - \bar{\mby})$ where $\mbLambda$ is the $Q \times G$ matrix whose rows are $\mblambda_q$ ordered by decreasing eigenvalue.

PCA is frequently used in computational genomics. It is commonly used for visualisation of genomic variation data such as single nucleotide polymorphism (SNP) datasets, where it has an elegant interpretation in terms of the underlying genealogical history of samples \cite{mcvean2009genealogical}.
It is often applied to microarray gene expression data for tasks such as data visualisation (see e.g. \cite{ringner2008principal}) and for clustering samples \cite{yeung2001principal}.

Since the advent of single-cell RNA-sequencing, PCA has been the go-to dimensionality reduction algorithm for exploratory data analysis\footnote{Though faces stiff competition from t-Stochastic Neighbour Embedding (tSNE)}. Examples include latent space projections to understand cell types and hierarchies in the developing lung \cite{treutlein2014reconstructing} and the transcriptional states defining differentiation of embryonic stem cells under different serums \cite{kolodziejczyk2015single}. PCA is also used for clustering cells, either as a preprocessing step prior to clustering algorithms like $k-$means or as part of likelihood-based clustering \cite{yau2016pcareduce}.

PCA also forms an initial step in many pseudotime algorithms such as in TSCAN \cite{ji2016tscan} and Waterfall \cite{shin2015single}. However, no studies have actually considered that a principal component of the data itself could \emph{be} the pseudotemporal trajectory. Intuitively such an idea is appealing since we would expect the pseudotemporal process to be the dominant source of variation within the data.

\subsection{Probabilistic principal components analysis}

One weakness of standard PCA is the absence of any probabilistic framework or interpretation. In a landmark paper \cite{tipping1999probabilistic} Tipping \& Bishop derived\footnote{By first considering a factor analysis model that we discuss in section \ref{sec:intr:fa}.} probabilistic PCA (PPCA) that provides an explicit generative model for PCA and relates the maximum likelihood estimates of parameters to the algorithmic estimation we discussed previously.

The generative model for PPCA is given by

\begin{equation}
  \begin{aligned}
    \mbz_n & \sim \Norm(\mbzero, \mbI) \\
    \mby_n & \sim \Norm(\mbLambda \mbz_n + \mbmu, \sigma^2 \mbI)
  \end{aligned}
\end{equation}

where $\mbmu$ is the expectation of $\mby$ and $\sigma^2$ is the measurement variance. In other words, if we centre $\mby$ so that $\text{E}[\mby] = 0$ then PPCA corresponds to a Gaussian noise model with a mean given by standard PCA and isotropic covariance. Tipping and Bishop futher derived closed form expressions for the maximum likelihood estimates (MLEs) of $\mbLambda$ and $\sigma^2$ along with the conditional distribution of the latent variables $p(\mbz_n | \mbLambda, \mby_n)$. They demonstrated that in the limit $\sigma^2 \rightarrow 0$ the MLE estimate of $\mbLambda$ is identical (up to arbitrary rotation) to that of standard PCA. Furthermore, the MLE estimate of $\sigma^2$ is given by

\begin{equation}
  \sigma^2_{\text{MLE}} = \frac{1}{G-Q} \sum_{j = Q + 1}^G \zeta_j
\end{equation}

where $\zeta_j$ are the eigenvalues of the sample covariance matrix ordered by decreasing size. In other words $\sigma^2$ is the variance ``lost'' in the projection, averaged over the remaining dimensions.

\subsection{Factor analysis} \label{sec:intr:fa}

Factor analysis (FA) precedes both PCA and PPCA, dating back to Spearman's work on ``general intelligence'' \cite{spearman1904general} (see section \ref{intr:fa_hist}). The overall model is very similar to that of PPCA with the only difference being the measurement (co-)variance diagonal rather than isotropic. This gives a generative factor analysis model of the form

\begin{equation} \label{eq:fa}
  \begin{aligned}
    \mbz_n & \sim \Norm(\mbzero, \mbI) \\
    \mby_n & \sim \Norm(\mbLambda \mbz_n + \mbmu, \mbSigma)
  \end{aligned}
\end{equation}

where $\mbSigma = \text{diag}(\sigma_1^2, \ldots, \sigma_G^2)$.

Maximum likelihood inference of factor analysis models may be performed using iterative procedures such as expectation-maximisation (EM) or Bayesian inference performed through MCMC methods or variational inference.

Factor analysis notably suffers from the \emph{rotation problem}. Given a $Q \times Q$ orthogonal rotation matrix $\mbR$ the likelihood is invariant under simultaneous rotation of both the factor matrix $\mbLambda$ and latent values $\mbz$\footnote{
Given measurement noise $\mbepsilon$ the likelihood is unchanged under $\mby = \mbLambda \mbz + \mbepsilon = \mbLambda \mbR \mbR^T \mbz + \mbepsilon = \mbLambda' \mbz' + \mbepsilon$ where $\mbLambda' = \mbLambda \mbR$ and $\mbz' = \mbR \mbz$ are the loadings and projections in the rotated space.
}. %Several solutions have been proposed to this
Note that if $Q=1$ the rotation problem is essentially a scaling problem (we can divide $\mbLambda$ by $k$ and multiply $\mbz$ by $k$ to achieve the same likelihood) but this is solved by fixing the scales of $\mbLambda$ and $\mbz$ through priors.

\subsubsection{Origins of factor analysis and connection to pseudotime} \label{intr:fa_hist}



\begin{table}
  \centering
\subfloat[School test scores for children across subjects]{\begin{tabular}{|c|ccc|}
\hline
Student & Maths & Physics & Biology\\
\hline
A & 8  & 9 & 7 \\
B & 3 & 1 & 5 \\
C & 6 & 6 & 8 \\
D & 4 & 2 & 7 \\
\hline
\end{tabular}}
\hspace{5pt}
\subfloat[Gene expression values of cells for transcription factors]{\begin{tabular}{|c|ccc|}
\hline
Cell & \emph{MYOD} & \emph{MYF5} & \emph{MYOG} \\
\hline
A & 8  & 9 & 7 \\
B & 3 & 1 & 5 \\
C & 6 & 6 & 8 \\
D & 4 & 2 & 7 \\
\hline
\end{tabular}}
\caption{Spearman noted that children's test scores were correlated across subjects (a). Cells' expression of genes in biological pathways is likewise correlated (b).} \label{tbl:fa}
\end{table}

Factor analysis was first introduced by Spearman in 1904 \cite{spearman1904general} by considering children's test scores in subjects such as that in table \ref{tbl:fa}(a). He noticed that the scores are correlated - for example, if a child has a high score in physics they are more likely to have a high score in maths, and vice versa. Spearman's insight was that rather than the scores being correlated with each other, they were correlated with an underlying (one-dimensional) hidden factor he termed \emph{general intelligence}. Mathematically this can be expressed as

\begin{equation}
\mby_n = \mblambda z_n + \epsilon_n
\end{equation}

where $\mby_n$ is the vector of child $n$'s test scores, $\mblambda$ is a vector of subject specific constants (the ``loadings''), $z_n$ is the ``general intelligence'' and $\epsilon_n$ is any noise not explained by the model\footnote{
Of course the idea of a single ``general intelligence'' goes against common sense. If there truly is a single latent factor it is possible to form the \emph{tetrad equations}. Later studies showed that deviations in the tetrad equations could not be explained by sampling noise alone. For an overview see \url{https://www.stat.cmu.edu/~cshalizi/350/lectures/12/lecture-12.pdf}.
}.

One can see in hindight the parallels to the pseudotime estimation problem: we measure some high-dimensional quantity $\mby_n$ over various subjects and have a hazy and somewhat under-defined one-dimensional generating process $z_n$. In the context of psychology, this process is intelligence: something unmeasurable that we use to make sense of observations in life such as performance in tests. Similarly in biology we have the concept of differentiation trajectories: something equally unmeasurable but which we indirectly observe through molecular measurements such as gene expression quantification.

Given the parallels between the original interpretation of factor analysis and differentiation trajectories we may suspect that it is particularly suited to the pseudotime inference. Indeed, chapters 3-5 are devoted to various modifications of factor analysis to infer such trajectories.

% \subsubsection{Nonlinear factor analysis}

% \subsection{Mixture models}


\subsection{Manifold learning}

While not strictly a class of statistical latent variable model, manifold learning has found much success in single-cell genomics. While methods such as PCA and FA attempt to infer low-dimensional \textbf{linear} subspaces embedded in high-dimensional space, the field of manifold learning extends this to the nonlinear setting. Several manifold learning algorithms applied to single-cell genomics are reviewed below and utilised in chapter 2.

\subsubsection{Laplacian eigenmaps}

Laplacian eigenmaps \cite{Belkin2003} are part of a larger class of \emph{spectral methods} including diffusion maps (section \ref{sec:diffusion_maps}). Starting with the $N \times G$ matrix $\mbY$, laplacian eigenmaps seeks a $N \times Q$-dimensional embedding $\mbZ$ with row vectors $\mbz_n$ for each cell through minimisation of the quantity

\begin{equation}
  \sum_{n,n'} W_{n,n'}\lvert \lvert \mbz_n - \mbz_{n'}\rvert \rvert^2
\end{equation}

subject to the constrain that $\mbz_q^T \mbz_q = 1 \; \forall q$ where $\mbz_q$ are the column vectors of $\mbz$.
 $\mbW$ is an $N \times N$ similarity matrix between the samples (cells) with the intuition that if
$W_{n,n'}$ is large then the distance between $\mbz_n$ and $\mbz_{n'}$ is heavily penalised placing them close together in the reduced space. Conversely, if $W_{n,n'}$ is small then large distances between them does not affect the optimisation problem. Solutions for $\mbZ$ can readily be found by solving an eigenvalue equation. % KC check + reference
Laplacian eigenmaps were used for pseudotime inference in the \texttt{embeddr} package \cite{campbell2015laplacian} using a symmetrised $k$-nearest neighbour graph for $\mbW$.


\subsubsection{Diffusion maps} \label{sec:diffusion_maps}

Diffusion maps are closely related to laplacian eigenmaps and have been successfully applied to single-cell RNA-seq data both in the context of visualisation \cite{haghverdi2015diffusion} and pseudotime inference \cite{haghverdi2016diffusion}. The basic idea is to consider points on the manifold in terms of a diffusion process, with a sample more likely to diffuse to one closer to it than further away. It begins by constructing a transition matrix

\begin{equation}
  P_{n',n} = \frac{1}{Z(\mby_{n'})} \exp\left( - \frac{\lvert \lvert \mby_n - \mby_{n'} \rvert \rvert^2}{2 \sigma^2}\right)
\end{equation}

where $Z(\mby_{n'}) = \sum_{n}  \exp\left( - \frac{\lvert \lvert \mby_n - \mby_{n'} \rvert \rvert^2}{2 \sigma^2}\right)$ and $\sigma^2$ is a characteristic length scale. $P_{n',n}$ can be thought of as the probability of transitioning or \emph{diffusing} from cell $n$ to $n'$. A renormalised transition matrix $\tilde{\mbP}$ can then be definied that takes into account the local density of samples in the space. One can then decompose the diffusion distances into a sum over the eigenvectors of $\tilde{\mbP}$
weighted by eigenvalues,
implying that retaining eigenvectors for the first $k$ ordered eigenvalues captures the major structure of the manifold and are therefore useful for visualisation. Haghverdi et al. \cite{haghverdi2015diffusion} further derive a heuristic for selection of the kernel width $\sigma$ in terms of the effective number of neighbours of each cell.

\subsubsection{Multidimensional scaling}

Multidimensional scaling (MDS) has previously been used for the visualisation of large genomic datasets \cite{tzeng2008multidimensional} and more recently was used as the initial dimensionality reduction step for the pseudotime algorithm \texttt{SCORPIUS} \cite{cannoodt2016scorpius}. It is motivated by the problem of trying to place cities on points on a map if we are given the distances between them. Given an $N \times N$ distance matrix $\mbD$ MDS attempts to minimise the quantity

\begin{equation}
  \texttt{Stress}(\mbZ) = \left(
  \sum_{n \neq n' = 1}^N (d_{nn'} - \lvert \lvert \mbz_n - \mbz_{n'} \rvert \rvert)^2
  \right)^{\frac{1}{2}}
\end{equation}

where $\mbz_n$ is the low dimensional embedding of sample $n$. The intuition is if $n$ and $n'$ are close we minimise the distance between  $\mbz_n$  $\mbz_{n'}$ and if $n$ and $n'$ are far apart we need to maximise the distance between  $\mbz_n$  and $\mbz_{n'}$ so that it is as close to $d_{nn'}$ as possible.

\subsubsection{Locally linear embedding} \label{sec:lle}

Locally linear embedding (LLE) was used successfully by \emph{SLICER} \cite{welch2016slicer} as a nonlinear dimensionality step after variable gene selection. LLE begins by defining an $N \times N$ weight matrix $\mbW$ where $W_{nn'}$ represents how useful sample $n'$ is for reconstructing $n$. An optimal $\mbW$ is found by minimising

\begin{equation}
  \sum_{n} \lvert \lvert \mby_n - \sum_{n'} W_{nn'} \mby_{n'} \rvert \rvert^2
\end{equation}

where $\mbW$ is sparsely constrained so that each point is only reconstructed by its $k$ nearest neighbours and so that the row sums of $\mbW$ are 1. The $Q$ (reduced) dimensional reconstructions $\mbz_n, \; n = 1, \ldots, N$ are then found via minimising

\begin{equation}
  C(\mbZ) = \sum_{n} \lvert \lvert \mbz_n - \sum_{n'} W_{nn'} \mbz_{n'} \rvert \rvert^2
\end{equation}

where $\mbW$ is kept fixed in the second optimisation step. In other words, we seek a low dimensional embedding such that the points have approximately the same relationship to each other in the reduced space as in the full ($G$-dimensional) space. Minimisation of $C(\mbZ)$ can subsequently be performed via a sparse eigenvalue problem.

\subsubsection{t-distributed stochastic neighbour embedding} \label{sec:tsne}

t-distributed stochastic neighbour embedding (t-SNE) \cite{maaten2008visualizing} has become incredibly popular for the visualisation of single-cell RNA-seq data and as the initial dimensionality step for several pseudotime algorithms. Similarly to diffusion maps\footnote{
Note that t-SNE differs in defining a different kernel width for each data point.
}, it begins by defining a conditional transition matrix

\begin{equation}
  P_{n'|n} = \frac{1}{Z(\mby_{n'})} \exp\left( - \frac{\lvert \lvert \mby_n - \mby_{n'} \rvert \rvert^2}{2 \sigma_n^2}\right)
\end{equation}

which can be interpreted as the probability under a Gaussian likelihood of $n$ choosing $n'$ as its neighbour. This is then symmetrised to form $P_{n'n} = \frac{1}{2N}(  P_{n'|n} +   P_{n|n'})$.

It then defines similarities in the latent space as

\begin{equation}
  Q_{nn'} = \frac{
  (1 + \lvert \lvert \mbz_n - \mbz_{n'} \rvert \rvert^2)^{-1}
  }{
  \sum_{m \neq m'} (1 + \lvert \lvert \mbz_m - \mbz_{m'} \rvert \rvert^2)^{-1}
  }
\end{equation}

which is equivalent to measuring distances in the latent space with a Student-t distribution with one degree of freedom. Values of $\mbz_n$ are found by minimising the Kullback–Leibler (KL) divergence
%from $\mbP$ to $\mbQ$
$\KL{\mbP}{\mbQ} = \sum_{n \neq n'} P_{nn'} \log \frac{P_{nn'}}{Q_{nn'}}$.

It is hard to overstate how popular t-SNE has been for visualising single-cell gene expression data. Examples include as the initial dimensionality reduction step in \texttt{SCUBA} \cite{marco2014bifurcation} or for visualisation of branch structure of single-cell mass cytometry data \cite{setty2016wishbone}. Criticisms of t-SNE include the required specification of the kernel widths $\sigma_n$ (which can be interpreted in terms of an effective number of nearest neighbours or \emph{perplexity}) and the number of iterations of the (stochastic) gradient descent aglorithm.


\subsection{Gaussian process latent variable models} \label{sec:gplvm}

Gaussian Process Latent Variable Models (GPLVM) are the subject of chapter 2 but are mentioned here for completeness. Technically GPLVM is a form of probabilistic manifold learning that learns an explicit map from the latent space to the observed space but may also be seen as a form of nonlinear factor analysis. In the factor analysis model of equation \ref{eq:fa} typical estimation proceeds by marginalising over $\mbz_n$ to give a marginal likelihood $\mby_n \sim(\mbmu, \mbLambda \mbLambda^T + \mbSigma)$ followed by direct optimisation. However, Lawrence \cite{lawrence2004gaussian} instead marginalised over the mapping $\mbLambda$ through a prior of the form $p(\mbLambda) = \prod_{q=1}^Q \Norm(\mblambda_q | \mbzero, \alpha^{-1} \mbI)$. This introduces a coupling between different samples $\mby_n$. Let $\mbY$ be the full $N \times G$ data matrix with row vectors $\mby_n$ and column vectors $\mby_g$ and $\mbZ$ the $N \times Q$
matrix of latent values with row vectors $\mbz_n$. The likelihood marginalised over the mapping is then

\begin{equation}
  p(\mbY) = \prod_{g = 1}^G \norm(\mby_g | \mbzero, \alpha^{-1} \mbZ \mbZ^T + \mbSigma)
\end{equation}

where $\mbSigma$ is the diagonal noise covariance matrix as before.

Lawrence's key insight was that the term $\alpha^{-1} \mbZ \mbZ^T$ in the covariance matrix represents similarity between difference samples, since the covariance between samples $n$ and $n'$ is $\alpha^{-1} \mbz_n \cdot \mbz_{n'}$. Therefore, it can be replaced with any positive definite \emph{kernel} $k(\mbz_n, \mbz_{n'})$ representing similarity between $\mbz_n$ and $\mbz_{n'}$. Popular examples include the squared exponential kernel

\begin{equation}
k_{\text{SQE}}(\mbz_n, \mbz_{n'}) = \sigma_f^2 \exp\left(-\frac{1}{2l^2} \lvert \lvert \mbz_n - \mbz_{n'} \rvert\rvert^2\right)
\end{equation}

as used in \cite{campbell2016order} or the Matern family such as the $\text{Matern}_{3/2}$ kernel used in \cite{reid2016pseudotime}:

\begin{equation}
  k_{\text{Matern}_{3/2}}(\mbz_n, \mbz_{n'}) =
  \left(
  1 + \sqrt{3} \lvert \mbz_n - \mbz_{n'} \rvert \right) \exp\left( - \sqrt{3} \lvert \mbz_n - \mbz_{n'} \rvert \right).
\end{equation}

GPLVM has been widely applied to single-cell expression data, including to single-cell qPCR data \cite{buettner2012novel} and single-cell RNA-seq \cite{campbell2016order,macaulay2016single}. Latent embeddings inferred using GPLVM are typically under-constrained leading to a number of studies that introduce ``data-driven priors'' to further constrain the model, such incorporating the t-SNE cost function to preserve local structure in GPLVM \cite{van2009preserving}

\section{Probabilistic modelling of genomic trajectories}

The overall aim of this thesis is a comprehensive probabilistic treatment of the pseudotime problem. Chapter two exchanges the post-dimensionality-reduction cell ordering algorithms of many pseudotime methods for probabilistic curves using Gaussian Process Latent Variable Models and asks what the effects are of incorporating uncertainties into downstream modelling. It also introduces a differential-expression-over-pseudotime model.
Chapter three introduces a generative model of pseudotime based on nonlinear factor analysis that replaces the dimensionality reduction and cell ordering steps with a single inference procedure.
Chapter four considers Bayesian inference of bifurcations in single-cell data using a hierarchical mixture of factor analysers.
Chapter five introduces a novel type of latent variable model that lets a secondary dataset to perturb the factor loadings in the first. This is applied to a diverse set of single-cell and bulk cancer studies and identifies novel interactions between phenotypic covariates and biological pathways.
Finally, chapter six incorporates a discussion of some of the weaknesses of probabilstic models of pseudotimes and suggests future directions.
