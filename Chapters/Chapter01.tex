%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

\section{Single-cell RNA-sequencing}

\subsection{Introduction}

It is hard to overstate the impact of single-cell whole-transcriptome expression quantification on recent scientific research. The first single-cell RNA-sequencing (RNA-seq) method was introduced in 2009 \cite{Tang2009-th}, and gained widespread interest around three years later with the introduction of methods such as Smart-seq \cite{Ramskold2012-wt} and Smart-seq2 \cite{Picelli2013-tm} that significantly reduced the cost of library preparation. It is quite incredible that in the space of a little over five years the technology has sufficiently advanced to move from profiling a few tens of cells to thousands and hundreds of thousands using technologies such as Drop-seq \cite{Macosko2015-ek} and the 10x platform \cite{Zheng2017-vj}.

This mass-production of single-cell data has led to an explosion of methods development for downstream analysis by both computational biologists and statisticians. There are now a wide range of methods specific to single-cell data for common analyses such as normalisation \cite{Lun2016-oj,Bacher2017-ga}, differential expression \cite{Kharchenko2014,Delmans2016-po,Korthauer2016-iz}, dimensionality reduction \cite{pierson2015zifa,Wang2017-dl}, and ``pseudotime'' analyses (see section \ref{sec:pseudotime}). Indeed, there are now over 100 dedicated tools for analysing single-cell data, 
the majority of which have been produced since 2015.

\subsection{Why quantify gene expression in single-cells?}

The basic logic behind quantifying gene expression in single cells is that we may expect some heterogeneity at the single cell level that is averaged over in bulk. For example, cells may undergo some biological process asychronously, where even time series analysis would average over progression at each point\footnote{Computational methods to correct for this (known as ``pseudotime ordering'') are the main focus of this thesis and are discussed in-depth in section \ref{sec:pseudotime})} \cite{Trapnell2014-xi}. Furthermore, multiple distinct cell types \cite{Kolodziejczyk2015-jd} or rare cell types \cite{Grun2015-xz} may exist within a population that a bulk assay would be unable to resolve. Some authors have even suggested that Simpson's paradox could mean that gene co-expression patterns observed in bulk are incorrect once groupings at the single-cell level are taken into account \cite{Trapnell2015-mz}.

% Applications in differentiation

Arguably one of the most popular applications of single-cell RNA-sequencing has been to differentiating cells, tracking the changes in gene regulation as the cells progress. Early examples include tracking the differentiation of primary human myoblasts \cite{Trapnell2014-xi}, which revealed switch-like changes in expression of key regulatory factors, and tracking the differentiation of alveolar cells in mice that allowed for the identification of bipotent progenitor cells \cite{Treutlein2014-ob}. Such ideas have been extended to many biological systems, such as differentiating hematopoietic stem cells \cite{zhou2016tracing,Kowalczyk2015-li}, sensory epithelial cells from the inner ear \cite{Burns2015}, and more exotic examples such as the differentiation of zebrafish thrombocytes \cite{Macaulay2016-vv} and neural stem cells in planarians (flatworms) \cite{molinaro2016silico}.

A further popular application of single-cell RNA-seq has been identifying subtypes of neurons (see e.g. \cite{Poulin2016-bl} for a full review). Possibly the first to do so was Usoskin et al. \cite{Usoskin2015-gp} who profiled 622 single mouse neurons from the dorsal root ganglion involved in the primary sensory system. Through a basic iterative PCA and clustering-by-eye procedure they identified 11 functionally distinct subtypes of primary sensory neurons. Shortly after Zeisel et al. \cite{Zeisel2015-cv} performed single-cell RNA-seq on 3005 cells from the mouse cerebral cortex. Rather than rely on the ad-hoc procedure of clustering-by-eye, they developed an iterative biclustering algorithm called BackSPIN. This suggested 9 major distinct cell types, which was further partitioned into 47 by repeating the clustering on each major cell type. When attempting to identify novel cell types based on gene expression it is obviously advantageous to sequence as many cells as possible. Not content with the poor scalability of existing procedures, Macosko et al. \cite{Macosko2015-ek} developed an entirely new library preparation procedure termed Drop-seq (see section \ref{sec:sc_lib_prep}) to cheaply profile thousands of cells at once. This allowed for the expression quantification of a staggering 44,808 cells from the mouse retina that were clustered into 39 transcriptionally distinct cell types using density-based clustering on a t-SNE projection. Although identifying cellular subtypes is a popular and intuitive idea, several issues remain. Surprisingly, there is no strict definition of a ``cell type'', a challenge to be tackled by the Human Cell Atlas \cite{Regev2017-sl}. Furthermore, practically every clustering algorithm - even those that choose the number of clusters ``automatically'' - have tunable parameters and modelling assumptions that leave the  final number of cell types discovered open to subjectivity.

A recent development in single-cell technologies has been the ability to simultaneously profile ``omics'' other than gene expression alone (see \cite{Macaulay2017-nn} for a full review). For example, G\&T-seq \cite{Macaulay2015-ok} allows for combined measurement of the genome and transcriptome in a single-cell. There are clear uses of such methods in cancer genomics, where the combined measurement of DNA mutations and RNA expression would allow for the quantification of clonality on gene expression and subsequently cellular regulation. Further methods have been used to jointly profile the methylation states of DNA at the single-cell level along with gene expression, known as scM\&T-seq 
\cite{Angermueller2016-ll,Hu2016-bz}. A recent publication has extended this to simultaneously measure chromatin accessibility, DNA methylation, and gene expression in single-cells \cite{Clark2017-pc}.

\subsection{Bulk expression quantification}



% \subsubsection{Gene expression microarrays}

The first mass-produced technology able to quantify transcriptome-wide gene expression was DNA microarrays. To infer nucleic acid abundance, DNA fragments (after reverse transcription from mRNA transcripts for gene expression quantification) bind to ``probes'' of known nucleic acid sequence and fluoresce strongly if a large quantity of nucleic acid with the correct complementary sequence is present.

However, gene expression microarrays suffer several disadvantages. The requirement of cDNAs to bind to probes of known sequence precludes de novo transcriptome construction, quantification of differential splicing, and any somatic variant calling. Furthermore, DNA microarrays are known to suffer from technical artefacts, such as positional effects where the spatial position of the probe affects the expression estimate\footnote{
This leads to the joke, ``what's the difference between white noise and microarrays? White noise doesn't contain artefacts.''
} (see e.g. \cite{petri2012detection,yu2006positional})

To overcome these issues researchers began direct sequencing of the cDNA fragments in a process known as RNA-sequencing (RNA-seq), which appeared in a trio of papers around the summer of 2008 \cite{mortazavi2008mapping,nagalakshmi2008transcriptional,lister2008highly}. RNA-seq normally begins with isolation of the RNA and depletion of any DNA using deoxyribonuclease to avoid genomic contamination. Optional enrichment of mRNAs may be performed using filtering for polyadenylation, before reverse transcription to cDNA and high-throughput (``next generation'') typically short read sequencing.


\subsection{Single-cell library preparation} \label{sec:sc_lib_prep}

Single-cell library preparation - the process of converting each cell's mRNA to barcoded cDNAs ready for sequencing - has come a long way since the manual isolation of Tang et al. in 2009 \cite{Tang2009-th}. The most popular methods - such as 
STRT-seq \cite{islam2011characterization}, 
Smart-seq \cite{Ramskold2012-wt}, 
Smart-seq2 \cite{Picelli2013-tm} and 
CEL-Seq \cite{hashimshony2012cel},
all follow similar principles. Cells are (typically FACS) sorted into 96 (and more recently 384) well plates. In each well cells are lysed to release the RNA, which is then reverse transcribed to cDNA. There is then typically some amount of amplification to increase the overall quantity of cDNA for more reliable sequencing. Next, each cell is uniquely barcoded by appending a known sequence of nucleic acid to each cDNA molecule. This step greatly increased the throughput of single-cell RNA-seq by allowing multiplexed sequencing of all cells at once. Finally, the individual libraries are ready for sequencing.



A popular commercial solution in the early days of single-cell RNA-seq was the Fluidigm C1 system that attempted to automate as far as possible the library preparation process. By using microfluidics, the C1 could prepare libraries using nanolitre reaction volumes which reduced reagent costs and improved the accuracy of expression quantification \cite{pollen2014low}. However, the C1 system notably suffered from low capture rates and ``doublets'' whereby two cells were captured and sequenced rather than one.

While such plate-based and microfluidic assays were incredibly successful, the single-cell isolation and barcoding steps precluded the majority of studies exceeding 1000 cells. However, this changed in 2015 with the introduction of ``droplet'' based technologies, namely Drop-seq \cite{Macosko2015-ek} and inDrop \cite{klein2015droplet}. In such assays cells are encapsulated in nanolitre droplets along with distinctly barcoded beads that allow for multiplexed sequencing. This allows for an incredible increase in the number of cells - the original Drop-seq publication sequenced 44,808 - and is currently being commercialised through companies such as 10x genomics \cite{Zheng2017-vj}.

\subsection{Features of scRNA-seq data}

\subsubsection{Mean-variance relationship}

One feature of single-cell RNA-seq data (and indeed RNA-seq data in general) is a non-trivial relationship between the variance of the expression and its mean. Accurate characterisation of this relationship is particularly crucial for small sample size datasets in order to robustly estimate the variance for statistical tests of differential expression \cite{Robinson2010-rj}. Statistical models of RNA-seq data typically fall into to camps when it comes to the distribution used for the likelihood. The first are the ``count-based'' methods  that act on the raw counts (scaled by size factors) using negative binomial distributions such as DESeq(2) and EdgeR \cite{Anders2010,Robinson2010-rj}. The second are the ``abundance-based'' methods  that log-transform\footnote{
Typically with some offset $c$ via $\log(x+c)$ to avoid divergence issues when $x \rightarrow 0$.
} the (normalised) counts and model using a Gaussian likelihood, such as Limma Voom and Sleuth \cite{Law2014-tu,Pimentel2016-xz}.

In the case of the count-based methods, RNA-seq data are  be well-approximated by negative binomial distributions \cite{Anders2010,Robinson2010-rj,Risso2017-qk} which relates the variance $\sigma^2$ to the mean $\mu$ of the counts for a given gene by

\begin{equation}
  \sigma^2 = \mu + \alpha \mu^2
\end{equation}

\begin{figure}
\centering
  \includegraphics[width=1.05\textwidth]{gfx/ch1/od-thesis.png}
  \caption[Mean-variance relationships in an example single-cell dataset.]{Mean variance relationships in an example single-cell dataset of thymic epithelial cells. Single-cell RNA-seq data shows a characteristic dependency on the $\text{CV}^2 = \frac{\sigma^2}
{\mu^2}$ on the mean counts for each gene (left). Methods such as \cite{Brennecke2013-xy} can then be used to identify genes that vary more than expected at random due to the technical noise in the dataset. An alternative view is to look at the mean variance trend in log space (right), where the overdispersed genes from \cite{Brennecke2013-xy} are often the most variable.} \label{fig:od}
\end{figure}


where $\alpha$ is a dispersion parameter and in the limit $\alpha \rightarrow 0$ a Poisson noise model is recovered. In other words the variance in expression increases quadratically with the mean number of counts for a particular gene. A common quantity to examine in such cases in the squared coefficient of variation $\text{CV}^2 = \frac{\sigma^2}{\mu^2}$ which for the negative binomial model implies $\text{CV}^2 = \alpha + \frac{1}{\mu}$. This relationship can be seen for a dataset of differentiating thymic epthilelial cells in figure \ref{fig:od}.

This relationship between the squared coefficient of variation and mean expression was exploited in a seminal single-cell RNA-seq paper \cite{Brennecke2013-xy} to identify genes that varied more than can be expected at random due to technical noise in the dataset. First a $\text{CV}^2-\text{mean}$ relationship is fitted for ERCC spike-ins\footnote{
ERCC spike-ins are known concentrations of known RNA sequence added to experiments to both calibrate the molecular concentrations to RNA-seq counts and to infer the limit of detection.
} to infer a null model of the variance relationship we expect in the data due to technical variation alone. A $p$-value for the coefficient of variation of endogenous (biological) genes given their mean expression can then be computed under the null model and subsequently genes ``overdispersed'' at some significance threshold can be identified. An example of this for differentiating thymic epthilelial cells can be seen in figure \ref{fig:od}, with the overdispersed genes shown in purple.

In the second set of ``abundance-based'' methods there is no analytical expression for the variance of the log-counts in terms of the mean since in general $\text{Var}[\log(X + c)] \neq \log(\text{Var}[X] + c)$. An example of this for the same set of thymic epithelial cells can be seen in figure \ref{fig:od}, coloured by the overdispersion test results from \cite{Brennecke2013-xy}. This forms a characteristic ``hump'' shape, with overdispersed genes typically exhibiting moderate mean expression but high variance in log space. Differential abundance methods such as Limma Voom \cite{Law2014-tu} and Sleuth \cite{Pimentel2016-xz} fit smoothed curves (such as LOESS curves) to obtain a nonparametric numerical approximation to the relationship. We use this formulation in chapter 3, where we note that ``interesting'' genes lie close enough to the diagonal to posit a linear mean-variance relationship in log space, i.e. $\sigma^2 = \phi \mu$.

\subsubsection{Dropout} \label{sec:intro_dropout}

An often discussed feature of single-cell RNA-seq data is the idea of \emph{dropout} where the probability a gene is detected in a cell is dependent on the mean expression of that gene. For each mRNA there is a stochastic probability of a failure to reverse transcribe into cDNA for sequencing. If only a few mRNAs exist for a given species (in other words, if a gene is lowly expressed) then this results as zero counts for that gene, rather than a small quantity detected. Obviously the more mRNAs present to begin with, the smaller the chance that reverse transcription misses all mRNAs of that species, inducing a dependence between a gene's (true) expression level and the dropout probability. This problem is further confounded by sequencing depth - if relatively few reads for a cell are sequenced then there is a high chance that a cDNA (which was lucky to be reverse transcribed in the first place) won't subsequently be sequenced. Thus typically the lower the read depth of a dataset the sparser the resulting count matrix is.

\begin{figure}
\centering
  \includegraphics[width=\textwidth]{gfx/ch1/thesis-dropout.png}
  \caption[Mean-dropout relationship in an example single-cell dataset.]{Relationship between dropout (proportion of cells not expressing a gene) and mean expression in an example single-cell dataset of thymic epithelial cells. Typically, the high variance genes lie along the right hand ``edge'' of the relationship (left). A number of models to account for dropout as a function of mean expression have been proposed, including exponential, double exponential and logistic (right).} \label{fig:intro_dropout}
\end{figure}

The characteristic pattern of proportion of cells in which a gene is expressed compared to the mean expression can be seen in figure \ref{fig:intro_dropout}A for an example dataset of thymic epithelial cells. Most single-cell datasets exhibit a set of genes that are not expressed (top-right corner), a set of housekeeping genes that are constitutively expressed at high levels, and a set of high-variance genes that contribute to heterogeneity in the dataset.

Several methods have attempted to correct for dropout through statistical modelling. An early example was SCDE \cite{Kharchenko2014} that uses a mixture model of a point pass at zero (the dropout component) and an ``amplified'' component. Noting that whether a gene is dropout is not a constant probability but dependent on the latent expression, they used a mixture-of-experts model where the probability of being amplified was logistic on the latent expression, i.e. $p_{\text{dropout}} = \text{Logistic}(\beta_0 + \beta_1 \mu)$ where $\mu$ is the latent expression and $\beta_0$ and $\beta_1$ are free parameters. An example of the logistic regression fit can be seen in figure \ref{fig:intro_dropout}B.

A different dropout model was implemented in the dimensionality reduction model ZIFA (Zero Inflated Factor Analysis, \cite{pierson2015zifa}). ZIFA models the probability of a dropout as $p_{\text{dropout}} = \exp(-\lambda x)$ for a ``dropout rate'' $\lambda$ and latent (true) expression $x$. A factor analysis model then operates on $x$ (see section \ref{sec:intr:fa}) to find a reduced dimension representation of the data accounting for dropout. A further model called M3Drop \cite{Andrews2016-ij} models dropout using Michaelis-Menten enzyme kinetics to select highly-variable, though the authors admit that their model is a specific case of SCDE.

Some methods take a different approach to dropout and treat it as a missing data problem that requires imputation. MAGIC \cite{Van_Dijk2017-wn} uses a diffusion-based approach (similar to maps, see section \ref{sec:diffusion_maps}) that computes a low-dimensional graph embedding before ``imputing'' each cell's expression by a weighted sum of its neighbours. A similar method SAVER \cite{SAVER} models expression using a Poisson likelihood with a latent true expression that is dependent on similar genes in the same cell (rather than the same gene in similar cells as per MAGIC).

\section{Pseudotime \& trajectories} \label{sec:pseudotime}


\subsection{The pseudotime estimation problem}

In many single-cell assays cells undergo some transformation over time. Examples include the differentiation of stem cells into neurons or skin cells, cells progressing through the cell cycle, or cells undergoing apoptosis (cell death). Ideally we would like to track expression changes over time, revealing key gene expression changes that correlate with the biological progression of interest.

However, most gene expression quantification methods to date - particularly transcriptome-wide ones such as single-cell RNA sequencing - destroy the cell during the measurement process, thus prohibiting repeated time-series measurement on the same cell. A first solution would be to repeatedly measure sets of cells at different time points, analogous to bulk RNA sequencing. However, the transcriptional heterogeneity at the single-cell level leads to an asynchronicity of expression \cite{Trapnell2014-xi}, meaning some cells at a given time point will be transcriptionally more similar to those at the next and some transcriptionally more similar to those at the previous.

\begin{figure}
\centering
  \includegraphics[width=0.98\textwidth]{gfx/ch1/figure1.png}
  \caption[The pseudotime estimation problem.]{The pseudotime estimation problem.
\textbf{A} Cells undergo some physical time process such as differentiation, cell-cycle, or apoptosis.
\textbf{B} The cell capture and expression quantification method - such as single-cell RNA-seq or single-cell mass-cytometry - leads to a loss of temporal information (if it were possible to measure in the first place).
\textbf{C} Pseudotime algorithms attempt to reconstruct the original time series using gene expression data alone.
\textbf{D} Downstream analysis such as differential expression proceeds with the pseudotimes in place of the physical times.
  } \label{fig:pseudotime}
\end{figure}

As a solution to these issues the idea of assigning cells a \emph{pseudotime} was proposed. In such a setting an algorithm takes the transcriptome-wide gene expression measurements for all cells and maps each on to a one-dimensional pseudotime\footnote{Mathematically, a pseudotime algorithm is just a function $f: \mathbb{R}^{N\times G} \rightarrow \mathbb{R}^N$ for $N$ cells and $G$ genes, mapping the gene expression matrix to a vector of pseudotimes (one for each cell). Strictly it is not a function $f: \mathbb{R}^G \rightarrow \mathbb{R}$ since the pseudotime for a given cell is (typically) dependent on all other cells. Such an explicit functional form reveals that pseudotime algorithms are in fact just a form of dimensionality reduction.}.
The pseudotime of a particular cell quantitatively represents that cell's progression through the biological process of interest and is not necessarily supposed to represent the physical (capture) time due to the transcriptional asynchronicity mentioned above. Consequently, each cell acts as a surrogate time point, the transcriptome of a given cell representing the supposed transcriptional signature of the biological process at that cell's assigned (pseudo-)time point. The set of cells ordered by pseudotime is often described as a \emph{trajectory} representing the continuous progression of transcription along the process of interest (figure \ref{fig:pseudotime}).

Several downstream analysis typically follow the assignment of pseudotimes to cells. Examples include identifying genes differentially expressed along the trajectory \cite{campbell2016switchde} or gene clusters differentially regulated \cite{Trapnell2014-xi}. However, care must be taken to realise that the pseudotimes are an uncertain estimate derived from exceptionally noisy input data and not a ``perfectly'' measured quantity such as physical capture time; this is the subject of chapter 2.

\subsection{Early applications to bulk expression data} \label{sec:int:early}

The ideas behind pseudotime orderings were first introduced in the context of ordering bulk microarray samples by Magwene et al. in 2002 \cite{Magwene2003-bm}. They examine the case of tracking gene expression from tumour samples in mouse models from disease inception and argue that ``the samples obtained from a mouse model of cancer do not represent a time series with a well-defined developmental order''
because early in cancer multiple tumours within the same tissue may progress asychronously. Magwene et al. therefore suggest that obtaining an ordering of samples based on the gene expression measurement alone may more accurately depict cancer development.

Their algorithm is motivated by the fact that the noise-free progression of microarray samples would trace out a smooth one-dimensional curve embedded in high dimensional expression space. They begin by fitting a minimum spanning tree\footnote{
A minimum spanning tree is a graph that connects all vertices together without any cycles using the minimum overall edge weight, a little like joining all the dots using the least ink possible.
} (MST) to the data using a modified distance function that is the ``standard pairwise dissimilarity'' if two points are ``relatively similar'' and the sum of the pairwise dissimilarities between two samples if ``relatively dissimilar''.

If the MST represents a path (i.e. it has no branches) then this is taken to be the ordering of cells. However, if the MST does have branches then the ``diameter path'' (the longest path through the MST) is computed and various heuristics are run to assess whether the diameter path essentially represents the dominant mode of variation through the data. If so then the diameter path is taken to be the ordering and if not a set of orderings representing the uncertainty in path variations is returned. Magwene et al. applied this algorithm to time series data of bacterial gene expression and found that it reconstructed the true time ordering of the samples accurately.

Gupta and Bar-Joseph (2008, \cite{Gupta2008-fd}) expanded on the ideas of Magwene et al. in three important ways. Firstly, they formally proved that a method could reconstruct the temporal order of expression profiling measurements. Interestingly, this proof shows that expression profiles of samples near each other in time will be more similar than those further apart in time, provided the change in expression is correlated over time\footnote{i.e. if a gene is upregulated at a given time point, it is more likely to be upregulated at the next and vice versa.} with high probability.
Secondly, they applied their method to static cancer data rather than simply recovering the capture time from time-series microarrays, though in practice the algorithm of Gupta et al. could be applied here also. Finally, their method recovered expression profiles for individual genes using spline fits. Interestingly, their model is close to a nonlinear factor analysis model like that considered in chapter 3, but performs a line search at each iteration of an expectation-maximisation (EM) algorithm to find the pseudotimes of each sample that minimises the mean squared error, rather than maximum likelihood or Bayesian inference of the pseudotimes.

Following this was the introduction of \emph{Sample Progression Discovery} (SPD) by Qui et al. in 2011 \cite{Qiu2011-ol}. This built on similar ideas of using MSTs to connect together microarray samples in a time ordering, with several important differences. SPD performs consensus clustering on the expression matrix to identify correlated gene expression modules and calculates a MST for each of them. It then chooses a subset of gene modules that have concordant MSTs and calculates an overall progression MST using only these. The authors emphasise that such an approach provides a feature selection ability, highlighting modules of genes associated with changes in the MST.


% \subsection{Single-cell trajectories}

\subsection{Single-cell pseudotime inference algorithms}

\subsubsection{Overview}

Ordering bulk microarray samples was arguably a niche area, with the three studies discussed above constituting the main work in the field. However, the advent of single-cell sequencing lead to an explosion of interest in the field, mostly due to the higher inter-sample heterogeneity present in single-cell data. The first single-cell pseudotime algorithm was published in March 2014 \cite{Trapnell2014-xi}; at time of writing (May 2017) there are now approximately 33 single-cell pseudotime inference algorithms that have at least been preprinted\footnote{
A useful spreadsheet of single-cell software is maintained at \url{https://tinyurl.com/mqabyur}.
}, a rate of roughly one per month.

\begin{sidewaystable}
  \centering
\begin{tabular}{|lccccc|}
\hline
Algorithm & Reference & Dimensionality reduction & Cell ordering & Probabilistic & Branching \\
\hline
Monocle & \cite{Trapnell2014-xi} & ICA\footnote{Independent component analysis} &
MST\footnote{Minimum spanning tree (cells are projected onto the longest path through the MST).} & No & Yes \\
Wanderlust & \cite{Bendall2014-rc} & N/A & KNN-graph\footnote{$k$-nearest neighbour graph. Trajectory inferred by an ensembl of random walks.} & No & No \\
Monocle 2 & \cite{Qiu2017-eu} & DDR-tree & Distance from root cell & No & Yes \\
Scuba & \cite{Marco2014-ug} & t-SNE\footnote{See section \ref{sec:tsne}} & Principal curves & No & Yes \\
Diffusion pseudotime & \cite{haghverdi2016diffusion} & N/A & Diffusion distance from root cell &
Interpretation\footnote{DPT has an interpretation in terms of the probability of one cell state transitioning into the other. However, it does not define a generative probabilistic model.} & Yes \\
SLICER & \cite{welch2016slicer} & LLE\footnote{See section \ref{sec:lle}} & Principal curves & No & Yes \\
DeLorean & \cite{Reid2016-yo} & N/A & GPLVM\footnote{See section \ref{sec:gplvm}} & Yes & No \\
TSCAN & \cite{Ji2016-gx} & PCA & Cluster-based MST & No & Yes \\
Waterfall & \cite{Shin2015} & PCA & Cluster-based distances & No & No \\
\rowcolor{Gray}
Ouija & \cite{Campbell2016-ys} & N/A & Nonlinear factor analysis & Yes & No \\
\rowcolor{Gray}
MFA & \cite{campbell2017probabilistic} & N/A & Mixture of factor analysers & Yes & Yes \\
\hline
\end{tabular}
\caption[An overview of some pseudotime algorithms.]{An overview of some pseudotime algorithms. Most involve a dimensionality reduction step followed by pseudotime assignment (``cell ordering'') in the reduced space, though arguably this constitutes a single dimensionality reduction step. Methods shaded in grey are introduced in this thesis.} \label{tbl:pseudotimecomparison}
\end{sidewaystable}

This renaissance was largely spurred by the development of \texttt{Monocle} \cite{Trapnell2014-xi}, discussed in detail in section \ref{tbl:monocle}. Monocle's two step procedure - an initial dimensionality reduction step using independent component analysis followed by a ``cell ordering'' step to order the cells using MSTs - greatly influenced the algorithms that followed. Examples include \texttt{TSCAN} that uses PCA for dimensionality reduction followed by clustering and MSTs for cell ordering; \texttt{embeddr} that uses laplacian eigenmaps for dimensionality reduction and principal curves for cell ordering; and \texttt{Waterfall} that uses PCA for dimensionality reduction followed by connecting clusters in the reduced space for cell ordering. An overview of some pseudotime algorithms is given in table \ref{tbl:pseudotimecomparison} with several important contributions discussed in detail in the following sections.

\subsubsection{Monocle} \label{tbl:monocle}

Monocle was designed to understand the gene expression dynamics that accompany the differentiation of stem cells into human skeletal muscle myoblasts (HSMM). There experimental design was to take stem cells and engineer a serum switch to induce differentiation, then perform single-cell RNA-sequencing at 0h, 24h, 48h and 72h afterwards. The authors noted that the expression patterns of key marker genes over time (such as switch-like inactivation of \emph{ID1}) was absent, and suggested asynchronicity of cellular development could be the underlying cause and thus a pseudotemporal ordering of cells was required.

Monocle begins with a gene selection step to identify which should be retained for ordering the cells. In the original publication the authors used those genes differentially expressed\footnote{
Using a Tobit likelihood model; see chapter 2 for further discussions on differential expression over pseudotime.
} between the time points, though emphasise that other approaches (such as selecting highly variable genes) would work also.

\begin{figure}
\centering
  \includegraphics[width=0.98\textwidth]{gfx/ch1/monocle.png}
  \caption[The Monocle pseudotime algorithm.]{The Monocle pseudotime algorithm (reused with permission).
\textbf{a} Dimensionality reduction is performed on the expression matrix using ICA. A MST is then fitted in the reduced space and cells are ordered along the diameter path, with branching also detected. Downstream analysis then includes differential expression and gene clustering.
\textbf{b} Monocle applied to human skeletal muscle myoblasts. The authors identified three cell types (proliferating cells, differentiating myoblasts, and interstitial mesenchymal cells). The main pseudotime trajectory progresses from the proliferating cells to the differentiating myoblasts, with the contaminating mesenchymal cells appearing on a separate branch.
  } \label{fig:monocle}
\end{figure}

The algorithm proceeds by using independent component analysis (ICA) to reduce the dimensionality of the dataset down to two dimensions. ICA attempts to find latent components in the data that are statistically independent and assumes that each component is non-Gaussian\footnote{
ICA finds a projection that maximises the kurtosis in the latent space, compared to PCA that maximises the variance (see section \ref{sec:intr:pca}).
}. 
%The original paper provides no justification for using ICA over any other dimensionality reduction algorithm, nor the choice of two latent dimensions though they do note that this makes it easier to visualise and interpret. Interestingly, the authors note
% \begin{displayquote}
% \emph{Reducing dimensionality of single-cell expression data amounts to describing
% each cell in terms of abstract sources, which are hidden variables that
% describe a cellâ€™s state but which are reflected in observed gene expression
% values.}
% \end{displayquote}
% which would seemingly justify a single dimensionality reduction step down to a one-dimensional latent component as a valid pseudotime algorithm.
 Monocle then proceeds with a secondary cell ordering step in the reduced space that largely follows the procedure of Magwene et al. \cite{Magwene2003-bm} (see section \ref{sec:int:early}) in constructing a MST in the reduced space and finding the ``diameter path'' (i.e. the longest path through the MST) as the trajectory. In order to find biological branches (rather than technical ones), the algorithm finds branches from ``indecisive'' vertices (those with degree greater than two) and returns the $k$ longest, where $k$ is an integer set by the user in line with prior knowledge of the expected number of terminally differentiated cells in the sample.

\subsubsection{Wanderlust}

Published just a month after Monocle was Wanderlust \cite{Bendall2014-rc}, an algorithm developed to uncover human B cell lymphopoiesis using single-cell mass cytometry data. Compared to transcriptome wide RNA sequencing, mass cytometry measures a smaller number of markers. In this study, a custom panel of 44 makers designed to characterise B-cells were measured, including ``phenotypic proteins, transcription factors, regulatory enzymes, cell-state indicators, and activation of regulatory signalling molecules''.

Wanderlust begins by constructing a $k$-nearest neighbour graph between all cells using euclidean distance and selects a small random subset of cells as ``waypoints''. It then creates a random ensemble of graphs by subsampling cells to mitigate the impact of ``short circuits'' on the trajectory construction\footnote{
Short circuits are defined as ``spurious edges between distant cells'' - two cells distant in pseudotime are connected due to random fluctuations in their gene expression. In theory such a problem will be worse in the ~40 dimensional mass cytometry data than 1000+ dimensional single-cell RNA-seq.
}. Wanderlust then assigns each cell an initial time as the shortest path through the graph from a manually chosen ``early'' cell, and performs an iterative procedure where each cells pseudotime is a weighted average of the distances to the waypoint cells, which is repeated until convergence. The overall pseudotime for each cell is taken as the average over   the ensemble of graphs.

\subsubsection{DeLorean}

DeLorean \cite{Reid2016-yo} was published approximately two years after Monocle and Wanderlust and is mentioned here as an example of an entirely different method. DeLorean departs from previous approaches in using a probabilistic model called a Gaussian Process Latent Variable Model (GPLVM, discussed in section \ref{sec:gplvm} and the subject of most of chapter 2).

The GPLVM learns a probabilistic mapping from the one dimensional latent pseudotimes to the observed gene expression profiles. The basic idea is that if two cells have similar pseudotimes then their transcriptomes will be highly correlated and thus will have similar measured expression profiles\footnote{
This is essentially a continuity assumption - that cells close in pseudotime will have expression profiles ``close'' in expression space. This assumption was discussed in \cite{Gupta2008-fd} (see section \ref{sec:int:early}) who showed the change in expression over (pseudo-)time must be correlated for it to be true. This logic underlies many if not all pseudotime inference methods, such as DPT \cite{haghverdi2016diffusion}.
}. Inference in the DeLorean model proceeds using the Stan probabilistic programming language, which performs automatic Bayesian inference using either Hamiltonian Monte Carlo (HMC) or Automatic-Differentiation Variational Inference (ADVI).

DeLorean notes several drawbacks, some of which  we also grapple with in chapter 2. It may only be used on time-series single-cell expression profiles, and thus in a sense is used to refine time series rather than a strict pseudotime algorithm. If $k_n$ is the capture time of cell $n$, then the prior on $n$'s pseudotime $t_n$ takes the form $t_n \sim \Norm(k_n, \sigma_n^2)$, so the confidence in the capture time $\sigma_n$ must also be set by the user. Furthermore, the characteristic length scale of the GPLVM kernel (see section \ref{sec:gplvm}) must be manually set by the user, and even so the posterior distribution of pseudotimes is multi-modal. Finally, GPLVM requires parameters for each output (gene) which can scale poorly, so in practice pseudotime fitting is limited to a small number of genes chosen \emph{a priori}.

However, a strength of DeLorean is the ability to discuss these drawbacks because of its formulation as a probabilistic model. Given the underlying continuity assumption common to all pseudotime algorithms we would expect they are all multimodal to some extent, or at least have multiple solutions that are dependent on algorithm parameters or initial values. However, their ad-hoc algorithmic nature precludes any assessment of this, while in the DeLorean model these can be recognised as a multimodal posterior distribution and steps taken to mitigate this such as the systematically incorporating prior information in the form of capture times to constrain the latent space.




\section{Statistical latent variable models}

\subsection{Principal component analysis} \label{sec:intr:pca}

Principal component analysis (PCA, \cite{jolliffe2002principal}) is a ubiquitous linear dimensionality reduction technique. PCA has several interpretations\footnote{Lior Pachter has an excellent blog post on this at \url{https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/}}, but the most common is that of a linear projection to a low-dimensional space such that the variance of the data points in the projected space is maximised. Intuitively, this can be thought of finding a linear subspace of the high-dimensional data space that best ``explains''
the data.

Mathematically, we start with $G$-dimensional data points $\{\mby_n\}, \; n = 1, \ldots, N$ and wish to find the $Q$ principal axes $\mblambda_q, \; q = 1, \ldots, Q$ that act as a mapping from the observed to latent space such that the variance of the input points projected to the latent space is maximal. To find $\mblambda_q$ the eigenvectors of the sample covariance matrix

\begin{equation}
  \mbS = \frac{1}{N}\sum_{n = 1}^N (\mby_n - \bar{\mby}) (\mby_n - \bar{\mby})^T
\end{equation}

are calculated where $\bar{\mby}$ is the sample mean of $\mby_n$. The $Q$ principal axes are then the $Q$ eigenvectors of $\mbS$ with the largest eigenvalues. The latent space projections $\mbz_n$ of each data point may then be obtained via $\mbz_n = \mbLambda (\mby_n - \bar{\mby})$ where $\mbLambda$ is the $Q \times G$ matrix whose rows are $\mblambda_q$ ordered by decreasing eigenvalue.

PCA is frequently used in computational genomics. It is commonly used for visualisation of genomic variation data such as single nucleotide polymorphism (SNP) datasets, where it has an elegant interpretation in terms of the underlying genealogical history of samples \cite{mcvean2009genealogical}.
It is often applied to microarray gene expression data for tasks such as data visualisation (see e.g. \cite{ringner2008principal}) and for clustering samples \cite{yeung2001principal}.

Since the advent of single-cell RNA-sequencing, PCA has been the go-to dimensionality reduction algorithm for exploratory data analysis\footnote{Though faces stiff competition from t-Stochastic Neighbour Embedding (tSNE).}. Examples include latent space projections to understand cell types and hierarchies in the developing lung \cite{Treutlein2014-ob} and the transcriptional states defining differentiation of embryonic stem cells under different serums \cite{kolodziejczyk2015single}. PCA is also used for clustering cells, either as a preprocessing step prior to clustering algorithms like $k-$means or as part of likelihood-based clustering \cite{yau2016pcareduce}.

PCA also forms an initial step in many pseudotime algorithms such as in TSCAN \cite{Ji2016-gx} and Waterfall \cite{Shin2015}. However, no studies have actually considered that a principal component of the data itself could \emph{be} the pseudotemporal trajectory. Intuitively such an idea is appealing since we would expect the pseudotemporal process to be the dominant source of variation within the data.

\subsection{Probabilistic principal components analysis}

One weakness of standard PCA is the absence of any probabilistic framework or interpretation. In a landmark paper \cite{tipping1999probabilistic} Tipping \& Bishop derived\footnote{By first considering a factor analysis model that we discuss in section \ref{sec:intr:fa}.} probabilistic PCA (PPCA) that provides an explicit generative model for PCA and relates the maximum likelihood estimates of parameters to the algorithmic estimation we discussed previously.

The generative model for PPCA is given by

\begin{equation}
  \begin{aligned}
    \mbz_n & \sim \Norm(\mbzero, \mbI) \\
    \mby_n & \sim \Norm(\mbLambda \mbz_n + \mbmu, \sigma^2 \mbI)
  \end{aligned}
\end{equation}

where $\mbmu$ is the expectation of $\mby$ and $\sigma^2$ is the isotropic measurement variance. In other words, if we centre $\mby$ so that $\text{E}[\mby] = 0$ then PPCA corresponds to a Gaussian noise model with a mean given by standard PCA and isotropic covariance. Tipping and Bishop further derived closed form expressions for the maximum likelihood estimates (MLEs) of $\mbLambda$ and $\sigma^2$ along with the conditional distribution of the latent variables $p(\mbz_n | \mbLambda, \mby_n)$. They demonstrated that in the limit $\sigma^2 \rightarrow 0$ the MLE estimate of $\mbLambda$ is identical (up to arbitrary rotation) to that of standard PCA. Furthermore, the MLE estimate of $\sigma^2$ is given by

\begin{equation}
  \sigma^2_{\text{MLE}} = \frac{1}{G-Q} \sum_{j = Q + 1}^G \zeta_j
\end{equation}

where $\zeta_j$ are the eigenvalues of the sample covariance matrix ordered by decreasing size. In other words $\sigma^2$ is the variance ``lost'' in the projection, averaged over the remaining dimensions.

\subsection{Factor analysis} \label{sec:intr:fa}

Factor analysis (FA) precedes both PCA and PPCA, dating back to Spearman's work on ``general intelligence'' \cite{spearman1904general} (see section \ref{intr:fa_hist}). The overall model is very similar to that of PPCA with the only difference being the measurement (co-)variance diagonal rather than isotropic. This gives a generative factor analysis model of the form

\begin{equation} \label{eq:fa}
  \begin{aligned}
    \mbz_n & \sim \Norm(\mbzero, \mbI) \\
    \mby_n & \sim \Norm(\mbLambda \mbz_n + \mbmu, \mbSigma)
  \end{aligned}
\end{equation}

where $\mbSigma = \text{diag}(\sigma_1^2, \ldots, \sigma_G^2)$.

Maximum likelihood inference of factor analysis models may be performed using iterative procedures such as expectation-maximisation (EM) or Bayesian inference performed through MCMC methods or variational inference.

Factor analysis notably suffers from the \emph{rotation problem}. Given a $Q \times Q$ orthogonal rotation matrix $\mbR$ the likelihood is invariant under simultaneous rotation of both the factor matrix $\mbLambda$ and latent values $\mbz$\footnote{
Given measurement noise $\mbepsilon$ the likelihood is unchanged under $\mby = \mbLambda \mbz + \mbepsilon = \mbLambda \mbR \mbR^T \mbz + \mbepsilon = \mbLambda' \mbz' + \mbepsilon$ where $\mbLambda' = \mbLambda \mbR$ and $\mbz' = \mbR \mbz$ are the loadings and projections in the rotated space.
}. %Several solutions have been proposed to this
Note that if $Q=1$ the rotation problem is essentially a scaling problem (we can divide $\mbLambda$ by $k$ and multiply $\mbz$ by $k$ to achieve the same likelihood) but this is solved by fixing the scales of $\mbLambda$ and $\mbz$ through priors.

\subsubsection{Origins of factor analysis and connection to pseudotime} \label{intr:fa_hist}



\begin{table}
  \centering
\subfloat[School test scores for children across subjects]{\begin{tabular}{|c|ccc|}
\hline
Student & Maths & Physics & Biology\\
\hline
A & 8  & 9 & 7 \\
B & 3 & 1 & 5 \\
C & 6 & 6 & 8 \\
D & 4 & 2 & 7 \\
\hline
\end{tabular}}
\hspace{5pt}
\subfloat[Gene expression values of cells for transcription factors]{\begin{tabular}{|c|ccc|}
\hline
Cell & \emph{MYOD} & \emph{MYF5} & \emph{MYOG} \\
\hline
A & 8  & 9 & 7 \\
B & 3 & 1 & 5 \\
C & 6 & 6 & 8 \\
D & 4 & 2 & 7 \\
\hline
\end{tabular}}
\caption{Children's test scores across subjects are correlated due to latent factors (a) in a similar 
way to the gene expression of key markers during differentiation (b).} \label{tbl:fa}
\end{table}

Factor analysis was first introduced by Spearman in 1904 \cite{spearman1904general} by considering children's test scores in subjects such as that in table \ref{tbl:fa}(a). He noticed that the scores are correlated - for example, if a child has a high score in physics they are more likely to have a high score in maths, and vice versa. Spearman's insight was that rather than the scores being correlated with each other, they were correlated with an underlying (one-dimensional) hidden factor he termed \emph{general intelligence}. Mathematically this can be expressed as

\begin{equation}
\mby_n = \mblambda z_n + \epsilon_n
\end{equation}

where $\mby_n$ is the vector of child $n$'s test scores, $\mblambda$ is a vector of subject specific constants (the ``loadings''), $z_n$ is the ``general intelligence'' and $\epsilon_n$ is any noise not explained by the model\footnote{
Of course the idea of a single ``general intelligence'' goes against common sense. If there truly is a single latent factor it is possible to form the \emph{tetrad equations}. Later studies showed that deviations in the tetrad equations could not be explained by sampling noise alone. For an overview see \url{https://www.stat.cmu.edu/~cshalizi/350/lectures/12/lecture-12.pdf}.
}.

One can see in hindsight the parallels to the pseudotime estimation problem: we measure some high-dimensional quantity $\mby_n$ over various samples and have a hazy and somewhat under-defined one-dimensional generating process $z_n$. In the context of psychology, this process is intelligence: something unmeasurable that we use to make sense of observations in life such as performance in tests. Similarly in biology we have the concept of differentiation trajectories: something equally unmeasurable but which we indirectly observe through molecular measurements such as gene expression quantification (table \ref{tbl:fa}(b)).

Given the parallels between the original interpretation of factor analysis and differentiation trajectories we may suspect that it is particularly suited to the pseudotime inference. Indeed, chapters 3-5 are devoted to various modifications of factor analysis to infer such trajectories.

% \subsubsection{Nonlinear factor analysis}

% \subsection{Mixture models}


\subsection{Manifold learning} \label{sec:int:manifold}

While not strictly a class of statistical latent variable model, manifold learning has found much success in single-cell genomics. While methods such as PCA and FA attempt to infer low-dimensional linear subspaces embedded in high-dimensional space, the field of manifold learning extends this to the nonlinear setting. Several manifold learning algorithms applied to single-cell genomics are reviewed below and utilised in chapter 2.

\subsubsection{Laplacian eigenmaps}

Laplacian eigenmaps \cite{Belkin2003} are part of a larger class of \emph{spectral methods} including diffusion maps (section \ref{sec:diffusion_maps}). Starting with the $N \times G$ matrix $\mbY$, laplacian eigenmaps seeks a $N \times Q$-dimensional embedding $\mbZ$ with row vectors $\mbz_n$ for each cell through minimisation of the quantity

\begin{equation}
  \sum_{n,n'} W_{n,n'}\lvert \lvert \mbz_n - \mbz_{n'}\rvert \rvert^2
\end{equation}

subject to the constrain that $\mbz_q^T \mbz_q = 1 \; \forall q$ where $\mbz_q$ are the column vectors of $\mbz$.
 $\mbW$ is an $N \times N$ similarity matrix between the samples (cells) with the intuition that if
$W_{n,n'}$ is large then the distance between $\mbz_n$ and $\mbz_{n'}$ is heavily penalised placing them close together in the reduced space. Conversely, if $W_{n,n'}$ is small then a large distance between $\mbz$ and $\mbz_{n'}$ has little effect on the optimisation problem. Solutions for $\mbZ$ can readily be found by solving an eigenvalue equation. % KC check + reference
Laplacian eigenmaps were used for pseudotime inference in the \texttt{embeddr} package \cite{campbell2015laplacian} using a symmetrised $k$-nearest neighbour graph for $\mbW$.


\subsubsection{Diffusion maps} \label{sec:diffusion_maps}

Diffusion maps are closely related to laplacian eigenmaps and have been successfully applied to single-cell RNA-seq data both in the context of visualisation \cite{Haghverdi2015} and pseudotime inference \cite{haghverdi2016diffusion}. The basic idea is to consider points on the manifold in terms of a diffusion process, with a sample more likely to diffuse to one closer to it than further away. It begins by constructing a transition matrix

\begin{equation}
  P_{n',n} = \frac{1}{Z(\mby_{n'})} \exp\left( - \frac{\lvert \lvert \mby_n - \mby_{n'} \rvert \rvert^2}{2 \sigma^2}\right)
\end{equation}

where $Z(\mby_{n'}) = \sum_{n}  \exp\left( - \frac{\lvert \lvert \mby_n - \mby_{n'} \rvert \rvert^2}{2 \sigma^2}\right)$ and $\sigma^2$ is a characteristic length scale. $P_{n',n}$ can be thought of as the probability of transitioning or \emph{diffusing} from cell $n$ to $n'$. A renormalised transition matrix $\tilde{\mbP}$ can then be defined that takes into account the local density of samples in the space. One can then decompose the diffusion distances into a sum over the eigenvectors of $\tilde{\mbP}$
weighted by eigenvalues,
implying that retaining eigenvectors for the first $k$ ordered eigenvalues captures the major structure of the manifold and are therefore useful for visualisation. Haghverdi et al. \cite{Haghverdi2015} further derive a heuristic for selection of the kernel width $\sigma$ in terms of the effective number of neighbours of each cell.

\subsubsection{Multidimensional scaling}

Multidimensional scaling (MDS) has previously been used for the visualisation of large genomic datasets \cite{tzeng2008multidimensional} and more recently was used as the initial dimensionality reduction step for the pseudotime algorithm \texttt{SCORPIUS} \cite{cannoodt2016scorpius}. It is motivated by the problem of trying to place cities on points on a map if we are given the distances between them. Given an $N \times N$ distance matrix $\mbD$ MDS attempts to minimise the quantity

\begin{equation}
  \text{Stress}(\mbZ) = \left(
  \sum_{n \neq n' = 1}^N (d_{nn'} - \lvert \lvert \mbz_n - \mbz_{n'} \rvert \rvert)^2
  \right)^{\frac{1}{2}}
\end{equation}

where $\mbz_n$ is the low dimensional embedding of sample $n$. The intuition is if $n$ and $n'$ are close we minimise the distance between  $\mbz_n$  $\mbz_{n'}$ and if $n$ and $n'$ are far apart we need to maximise the distance between  $\mbz_n$  and $\mbz_{n'}$ so that it is as close to $d_{nn'}$ as possible.

\subsubsection{Locally linear embedding} \label{sec:lle}

Locally linear embedding (LLE) was used successfully by \texttt{SLICER} \cite{welch2016slicer} as a nonlinear dimensionality step after highly-variable gene selection. LLE begins by defining an $N \times N$ weight matrix $\mbW$ where $W_{nn'}$ represents how useful sample $n'$ is for reconstructing $n$. An optimal $\mbW$ is found by minimising

\begin{equation}
  \sum_{n} \lvert \lvert \mby_n - \sum_{n'} W_{nn'} \mby_{n'} \rvert \rvert^2
\end{equation}

where $\mbW$ is sparsely constrained so that each point is only reconstructed by its $k$ nearest neighbours and so that the row sums of $\mbW$ are 1. The $Q$ (reduced) dimensional reconstructions $\mbz_n, \; n = 1, \ldots, N$ are then found via minimising

\begin{equation}
  C(\mbZ) = \sum_{n} \lvert \lvert \mbz_n - \sum_{n'} W_{nn'} \mbz_{n'} \rvert \rvert^2
\end{equation}

where $\mbW$ is kept fixed in the second optimisation step. In other words, we seek a low dimensional embedding such that the points have approximately the same relationship to each other in the reduced space as in the full ($G$-dimensional) space. Minimisation of $C(\mbZ)$ can subsequently be performed via a sparse eigenvalue problem.

\subsubsection{t-distributed stochastic neighbour embedding} \label{sec:tsne}

t-distributed stochastic neighbour embedding (t-SNE) \cite{maaten2008visualizing} has become incredibly popular for the visualisation of single-cell RNA-seq data and as the initial dimensionality step for several pseudotime algorithms. Similarly to diffusion maps\footnote{
Note that t-SNE differs in defining a different kernel width for each data point.
}, it begins by defining a conditional transition matrix

\begin{equation}
  P_{n'|n} = \frac{1}{Z(\mby_{n'})} \exp\left( - \frac{\lvert \lvert \mby_n - \mby_{n'} \rvert \rvert^2}{2 \sigma_n^2}\right)
\end{equation}

which can be interpreted as the probability under a Gaussian likelihood of $n$ choosing $n'$ as its neighbour. This is then symmetrised to form $P_{n'n} = \frac{1}{2N}(  P_{n'|n} +   P_{n|n'})$.

It then defines similarities in the latent space as

\begin{equation}
  Q_{nn'} = \frac{
  (1 + \lvert \lvert \mbz_n - \mbz_{n'} \rvert \rvert^2)^{-1}
  }{
  \sum_{m \neq m'} (1 + \lvert \lvert \mbz_m - \mbz_{m'} \rvert \rvert^2)^{-1}
  }
\end{equation}

which is equivalent to measuring distances in the latent space with a Student-t distribution with one degree of freedom. Values of $\mbz_n$ are found by minimising the Kullbackâ€“Leibler (KL) divergence
%from $\mbP$ to $\mbQ$
$\KL{\mbP}{\mbQ} = \sum_{n \neq n'} P_{nn'} \log \frac{P_{nn'}}{Q_{nn'}}$.

It is hard to overstate how popular t-SNE has been for visualising single-cell gene expression data. Examples include as the initial dimensionality reduction step in \texttt{SCUBA} \cite{marco2014bifurcation} or for visualisation of branch structure of single-cell mass cytometry data \cite{setty2016wishbone}. Criticisms of t-SNE include the required specification of the kernel widths $\sigma_n$ (which can be interpreted in terms of an effective number of nearest neighbours or \emph{perplexity}) and the number of iterations of the (stochastic) gradient descent algorithm.


\subsection{Gaussian process latent variable models} \label{sec:gplvm}

Gaussian Process Latent Variable Models (GPLVM) are the subject of chapter 2 but are mentioned here for completeness. Technically GPLVM is a form of probabilistic manifold learning that learns an explicit map from the latent space to the observed space but may also be seen as a form of nonlinear factor analysis. In the factor analysis model of equation \ref{eq:fa} typical estimation proceeds by marginalising over $\mbz_n$ to give a marginal likelihood $\mby_n \sim(\mbmu, \mbLambda \mbLambda^T + \mbSigma)$ followed by direct optimisation. However, Lawrence \cite{lawrence2004gaussian} instead marginalised over the mapping $\mbLambda$ through a prior of the form $p(\mbLambda) = \prod_{q=1}^Q \Norm(\mblambda_q | \mbzero, \alpha^{-1} \mbI)$. This introduces a coupling between different samples $\mby_n$. Let $\mbY$ be the full $N \times G$ data matrix with row vectors $\mby_n$ and column vectors $\mby_g$ and $\mbZ$ the $N \times Q$
matrix of latent values with row vectors $\mbz_n$. The likelihood marginalised over the mapping is then

\begin{equation}
  p(\mbY) = \prod_{g = 1}^G \norm(\mby_g | \mbzero, \alpha^{-1} \mbZ \mbZ^T + \mbSigma)
\end{equation}

where $\mbSigma$ is the diagonal noise covariance matrix as before.

Lawrence's key insight was that the term $\alpha^{-1} \mbZ \mbZ^T$ in the covariance matrix represents similarity between difference samples, since the covariance between samples $n$ and $n'$ is $\alpha^{-1} \mbz_n \cdot \mbz_{n'}$. Therefore, it can be replaced with any positive definite \emph{kernel} $k(\mbz_n, \mbz_{n'})$ representing similarity between $\mbz_n$ and $\mbz_{n'}$. Popular examples include the squared exponential kernel

\begin{equation}
k_{\text{SQE}}(\mbz_n, \mbz_{n'}) = \sigma_f^2 \exp\left(-\frac{1}{2l^2} \lvert \lvert \mbz_n - \mbz_{n'} \rvert\rvert^2\right)
\end{equation}

as used in \cite{campbell2016order} or the Matern family such as the $\text{Matern}_{3/2}$ kernel used in \cite{Reid2016-yo}:

\begin{equation}
  k_{\text{Matern}_{3/2}}(\mbz_n, \mbz_{n'}) =
  \left(
  1 + \sqrt{3} \lvert \mbz_n - \mbz_{n'} \rvert \right) \exp\left( - \sqrt{3} \lvert \mbz_n - \mbz_{n'} \rvert \right).
\end{equation}

GPLVM has been widely applied to single-cell expression data, including to single-cell qPCR data \cite{buettner2012novel} and single-cell RNA-seq \cite{campbell2016order,macaulay2016single}. Latent embeddings inferred using GPLVM are typically under-constrained leading to a number of studies that introduce ``data-driven priors'' to further constrain the model, such as incorporating the t-SNE cost function to preserve local structure in GPLVM \cite{van2009preserving}

\section{Probabilistic modelling of genomic trajectories}

The overall aim of this thesis is a comprehensive probabilistic treatment of the pseudotime problem. Chapter two exchanges the post-dimensionality-reduction cell ordering algorithms of many pseudotime methods for probabilistic curves using Gaussian Process Latent Variable Models and asks what the effects are of incorporating uncertainties into downstream modelling. It also introduces a differential-expression-over-pseudotime model.
Chapter three introduces a generative model of pseudotime based on nonlinear factor analysis that replaces the dimensionality reduction and cell ordering steps with a single inference procedure.
Chapter four considers Bayesian inference of bifurcations in single-cell data using a hierarchical mixture of factor analysers.
Chapter five introduces a novel type of latent variable model that lets a secondary dataset to perturb the factor loadings in the first. This is applied to a diverse set of single-cell and bulk cancer studies and identifies novel interactions between phenotypic covariates and biological pathways.
Finally, chapter six incorporates a discussion of some of the weaknesses of probabilistic models of pseudotimes and suggests future directions.
